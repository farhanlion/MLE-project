{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483c2ccd-b038-4289-a22e-58abab5bce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üì¶ Imports and setup\n",
    "# ===============================\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712b7677-8ecc-4f9e-bbf3-1502f3ad36ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/03 06:22:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ‚öôÔ∏è Initialize Spark\n",
    "# ===============================\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"gold_feature_store_daily\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4563f2ac-fb2e-4f14-81d7-aab2f0dfd02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Silver layer tables...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# üì• Load Silver Tables\n",
    "# ===============================\n",
    "\n",
    "print(\"Loading Silver layer tables...\\n\")\n",
    "\n",
    "df_userlogs = spark.read.parquet(\"/app/datamart/silver/user_logs\")\n",
    "df_transactions = spark.read.parquet(\"/app/datamart/silver/transactions\")\n",
    "df_latest_transactions = spark.read.parquet(\"/app/datamart/silver/latest_transactions\")\n",
    "df_members = spark.read.parquet(\"/app/datamart/silver/members\")\n",
    "txn_snapshots = (spark.read\n",
    "                  .option(\"header\", True)\n",
    "                  .option(\"inferSchema\", True)\n",
    "                  .parquet(\"/app/datamart/silver/max_expiry_transactions\"))\n",
    "    \n",
    "print(\"Done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43fd86e2-e8c5-4afa-9c55-4d18b3dcb7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=================================>                      (12 + 8) / 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Registered users for all dates: 20146895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# üë§ Registered Users\n",
    "# ===============================\n",
    "\n",
    "snapshot_users = txn_snapshots.select(\"snapshot_date\", \"msno\").distinct()\n",
    "snapshot_users = snapshot_users.join(df_members, on=\"msno\", how=\"left\")\n",
    "print(f\"üìÑ Registered users for all dates: {snapshot_users.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498aa8a2-4ffb-4a7c-ae8c-a873fbae81b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+------+------+-------+-------+-------+----------+----+-----+\n",
      "|                msno|      date|num_25|num_50|num_75|num_985|num_100|num_unq|total_secs|year|month|\n",
      "+--------------------+----------+------+------+------+-------+-------+-------+----------+----+-----+\n",
      "|kvfTVgxOfjbVBTXyY...|2016-11-26|     3|     0|     1|      0|     27|     30|  7254.569|2016|   11|\n",
      "|TKjom9SvWQfr9/FaS...|2016-11-09|     0|     0|     0|      0|      5|      4|    1246.0|2016|   11|\n",
      "|V7mwW25pIzSyhRvKt...|2016-11-02|     3|     0|     0|      0|      7|     10|   1529.59|2016|   11|\n",
      "|HpsUB9oBFNEkMz8LS...|2016-11-18|    13|     2|     2|      0|     21|     32|  6463.842|2016|   11|\n",
      "|kWl8bmqhgiRnkvlGe...|2016-11-18|     2|     1|     1|      1|      8|     13|  2557.191|2016|   11|\n",
      "+--------------------+----------+------+------+------+-------+-------+-------+----------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_userlogs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "571c257a-3188-4032-8295-e79b033c08d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==============================================>         (10 + 2) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+--------------+-----------------+-----------------------+-------------------+-------------------+--------+-------+--------------+--------------+\n",
      "|                msno|snapshot_date|city_clean|registered_via|registration_date|tenure_days_at_snapshot|registered_via_freq|          city_freq|city_idx|via_idx|       city_oh|        via_oh|\n",
      "+--------------------+-------------+----------+--------------+-----------------+-----------------------+-------------------+-------------------+--------+-------+--------------+--------------+\n",
      "|++MnSN1aZo9raWdDY...|   2015-08-14|         1|             7|       2013-05-21|                   1379|0.11904841041540457| 0.7097045811394772|     0.0|    3.0|(21,[0],[1.0])|(18,[3],[1.0])|\n",
      "|++UyRqjARgvFXB6Yd...|   2016-07-21|         5|             7|       2014-11-24|                    827|0.11904841041540457|0.05688315766973293|     1.0|    3.0|(21,[1],[1.0])|(18,[3],[1.0])|\n",
      "|++UyRqjARgvFXB6Yd...|   2015-11-23|         5|             7|       2014-11-24|                    827|0.11904841041540457|0.05688315766973293|     1.0|    3.0|(21,[1],[1.0])|(18,[3],[1.0])|\n",
      "|++qB56h2/tpY147Es...|   2016-12-31|         1|             7|       2015-08-31|                    547|0.11904841041540457| 0.7097045811394772|     0.0|    3.0|(21,[0],[1.0])|(18,[3],[1.0])|\n",
      "|++qTmh4qA8N9/jpTo...|   2015-12-27|         5|             7|       2011-05-27|                   2104|0.11904841041540457|0.05688315766973293|     1.0|    3.0|(21,[1],[1.0])|(18,[3],[1.0])|\n",
      "+--------------------+-------------+----------+--------------+-----------------+-----------------------+-------------------+-------------------+--------+-------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "snapshot_users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f9e38-470c-49c4-8803-fe8f7827a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:(129 + 12) / 158][Stage 30:>  (0 + 0) / 14][Stage 31:> (0 + 0) / 158]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# --- 0) Hygiene: ensure date types\n",
    "df_userlogs    = df_userlogs.withColumn(\"date\", F.to_date(\"date\"))\n",
    "snapshot_users = snapshot_users.withColumn(\"snapshot_date\", F.to_date(\"snapshot_date\"))\n",
    "\n",
    "# Aliases\n",
    "u = df_userlogs.alias(\"u\")\n",
    "s = snapshot_users.select(\"msno\", \"snapshot_date\").alias(\"s\")\n",
    "\n",
    "# --- 1) Date windows relative to snapshot_date\n",
    "start_30 = F.date_sub(F.col(\"s.snapshot_date\"), 30)\n",
    "end_30   = F.date_sub(F.col(\"s.snapshot_date\"), 1)    # include snapshot day: use F.col(\"s.snapshot_date\")\n",
    "start_7  = F.date_sub(F.col(\"s.snapshot_date\"), 7)\n",
    "end_7    = F.date_sub(F.col(\"s.snapshot_date\"), 1)\n",
    "\n",
    "# --- 2) Build windowed userlogs\n",
    "userlogs_30d = (\n",
    "    u.join(\n",
    "        s,\n",
    "        (F.col(\"u.msno\") == F.col(\"s.msno\")) &\n",
    "        (F.col(\"u.date\").between(start_30, end_30)),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "userlogs_7d = (\n",
    "    u.join(\n",
    "        s,\n",
    "        (F.col(\"u.msno\") == F.col(\"s.msno\")) &\n",
    "        (F.col(\"u.date\").between(start_7, end_7)),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "metrics = [\"num_25\",\"num_50\",\"num_75\",\"num_985\",\"num_100\",\"num_unq\",\"total_secs\"]\n",
    "\n",
    "# --- 3) 30d aggregates per (msno, snapshot_date)\n",
    "agg_30d = (\n",
    "    userlogs_30d\n",
    "    .groupBy(F.col(\"s.msno\").alias(\"msno\"), F.col(\"s.snapshot_date\").alias(\"snapshot_date\"))\n",
    "    .agg(\n",
    "        *[F.sum(F.col(f\"u.{m}\")).alias(f\"{m}_w30_sum\") for m in metrics],\n",
    "        F.countDistinct(F.col(\"u.date\")).alias(\"active_days_w30\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Completion rate (30d) = sum(num_100) / max(sum(num_unq), 1)\n",
    "agg_30d = agg_30d.withColumn(\n",
    "    \"complete_rate_w30\",\n",
    "    F.col(\"num_100_w30_sum\") / F.when(F.col(\"num_unq_w30_sum\") > 0, F.col(\"num_unq_w30_sum\")).otherwise(F.lit(1))\n",
    ")\n",
    "\n",
    "# Keep a convenience alias for seconds (30d)\n",
    "agg_30d = agg_30d.withColumnRenamed(\"total_secs_w30_sum\", \"sum_secs_w30\")\n",
    "\n",
    "# --- 4) 7d aggregates per (msno, snapshot_date)\n",
    "agg_7d = (\n",
    "    userlogs_7d\n",
    "    .groupBy(F.col(\"s.msno\").alias(\"msno\"), F.col(\"s.snapshot_date\").alias(\"snapshot_date\"))\n",
    "    .agg(F.sum(F.col(\"u.total_secs\")).alias(\"sum_secs_w7\"))\n",
    ")\n",
    "\n",
    "# --- 5) Engagement ratio 7/30\n",
    "agg_7_30 = (\n",
    "    agg_30d.select(\"msno\",\"snapshot_date\",\"sum_secs_w30\")\n",
    "    .join(agg_7d, [\"msno\",\"snapshot_date\"], \"left\")\n",
    "    .withColumn(\n",
    "        \"engagement_ratio_7_30\",\n",
    "        F.col(\"sum_secs_w7\") / F.when(F.col(\"sum_secs_w30\") > 0, F.col(\"sum_secs_w30\")).otherwise(F.lit(1))\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 6) Days since last play (max date <= snapshot_date)\n",
    "last_play = (\n",
    "    u.join(s, (F.col(\"u.msno\")==F.col(\"s.msno\")) & (F.col(\"u.date\") <= F.col(\"s.snapshot_date\")), \"inner\")\n",
    "     .groupBy(F.col(\"s.msno\").alias(\"msno\"), F.col(\"s.snapshot_date\").alias(\"snapshot_date\"))\n",
    "     .agg(F.max(F.col(\"u.date\")).alias(\"last_play_date\"))\n",
    "     .withColumn(\"days_since_last_play\", F.datediff(F.col(\"snapshot_date\"), F.col(\"last_play_date\")))\n",
    ")\n",
    "\n",
    "# --- 7) Trend in total_secs over 30d using slope ‚âà cov(day_idx, daily_secs)/var(day_idx)\n",
    "daily_secs_30d = (\n",
    "    userlogs_30d\n",
    "    .groupBy(F.col(\"s.msno\").alias(\"msno\"), F.col(\"s.snapshot_date\").alias(\"snapshot_date\"), F.col(\"u.date\").alias(\"date\"))\n",
    "    .agg(F.sum(F.col(\"u.total_secs\")).alias(\"daily_secs\"))\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"msno\",\"snapshot_date\").orderBy(\"date\")\n",
    "trend_30d = (\n",
    "    daily_secs_30d\n",
    "    .withColumn(\"day_idx\", F.row_number().over(w))  # 1..N\n",
    "    .groupBy(\"msno\",\"snapshot_date\")\n",
    "    .agg(\n",
    "        (F.covar_pop(\"day_idx\",\"daily_secs\") / F.var_pop(\"day_idx\")).alias(\"trend_secs_w30\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 8) Assemble features per (msno, snapshot_date)\n",
    "features = (\n",
    "    agg_30d\n",
    "    .join(agg_7d, [\"msno\",\"snapshot_date\"], \"left\")\n",
    "    .join(agg_7_30.select(\"msno\",\"snapshot_date\",\"engagement_ratio_7_30\"), [\"msno\",\"snapshot_date\"], \"left\")\n",
    "    .join(last_play.select(\"msno\",\"snapshot_date\",\"days_since_last_play\"), [\"msno\",\"snapshot_date\"], \"left\")\n",
    "    .join(trend_30d, [\"msno\",\"snapshot_date\"], \"left\")\n",
    ")\n",
    "\n",
    "# Optional: fill nulls for numeric outputs\n",
    "fill_map = {\n",
    "    \"sum_secs_w30\": 0.0,\n",
    "    \"active_days_w30\": 0,\n",
    "    \"complete_rate_w30\": 0.0,\n",
    "    \"sum_secs_w7\": 0.0,\n",
    "    \"engagement_ratio_7_30\": 0.0,\n",
    "    \"days_since_last_play\":  0.0,   # keep None if never played before snapshot\n",
    "    \"trend_secs_w30\": 0.0\n",
    "}\n",
    "features = features.na.fill({k:v for k,v in fill_map.items() if v is not None})\n",
    "\n",
    "# --- 9) Join back to snapshot_users (aka registered_users)\n",
    "registered_users = snapshot_users.alias(\"su\").join(\n",
    "    features, on=[\"msno\",\"snapshot_date\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# If you also want all the 30d sums for other metrics kept:\n",
    "# They are already present in agg_30d as <metric>_w30_sum, e.g., num_25_w30_sum, ...\n",
    "# You can select columns as needed:\n",
    "registered_users.select(\n",
    "    \"msno\",\"snapshot_date\",\n",
    "    \"sum_secs_w30\",\"active_days_w30\",\"complete_rate_w30\",\n",
    "    \"sum_secs_w7\",\"engagement_ratio_7_30\",\"days_since_last_play\",\"trend_secs_w30\"\n",
    ").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a388514-b6eb-48e6-b6ac-0d9300a6c351",
   "metadata": {},
   "source": [
    "Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153723b9-e296-4d6c-a791-2ed6ab25f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure date types\n",
    "df_transactions = df_transactions.withColumn(\"transaction_date\", F.to_date(\"transaction_date\"))\n",
    "registered_users = registered_users.withColumn(\"snapshot_date\", F.to_date(\"snapshot_date\")) \\\n",
    "                                   .withColumn(\"registration_date\", F.to_date(\"registration_date\"))\n",
    "\n",
    "# --- Join transactions to snapshots and keep only tx up to the snapshot_date\n",
    "t = df_transactions.alias(\"t\")\n",
    "r = registered_users.select(\"msno\",\"snapshot_date\",\"registration_date\").alias(\"r\")\n",
    "\n",
    "tx_asof_snap = (\n",
    "    t.join(r, F.col(\"t.msno\")==F.col(\"r.msno\"), \"inner\")\n",
    "     .where(F.col(\"t.transaction_date\") <= F.col(\"r.snapshot_date\"))\n",
    ")\n",
    "\n",
    "# ========= Latest transaction AS OF snapshot_date (per msno, snapshot_date)\n",
    "w_latest = Window.partitionBy(\"r.msno\",\"r.snapshot_date\").orderBy(F.col(\"t.transaction_date\").desc())\n",
    "latest_tx = (\n",
    "    tx_asof_snap\n",
    "    .withColumn(\"rn\", F.row_number().over(w_latest))\n",
    "    .where(F.col(\"rn\")==1)\n",
    "    .select(\n",
    "        F.col(\"r.msno\").alias(\"msno\"),\n",
    "        F.col(\"r.snapshot_date\").alias(\"snapshot_date\"),\n",
    "        F.col(\"t.transaction_date\").alias(\"latest_transaction_date\"),\n",
    "        F.col(\"t.is_auto_renew\").alias(\"last_is_auto_renew\"),\n",
    "        F.col(\"t.plan_list_price\").alias(\"last_plan_list_price\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Tenure days relative to latest tx (as of snapshot)\n",
    "tenure_asof = latest_tx.join(\n",
    "    r.select(F.col(\"msno\"), F.col(\"snapshot_date\"), F.col(\"registration_date\")),\n",
    "    [\"msno\",\"snapshot_date\"],\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"tenure_days\",\n",
    "    F.datediff(F.col(\"latest_transaction_date\"), F.col(\"registration_date\"))\n",
    ")\n",
    "\n",
    "# ========= Auto-renew stats AS OF snapshot_date (per msno, snapshot_date)\n",
    "auto_renew_stats = (\n",
    "    tx_asof_snap\n",
    "    .groupBy(F.col(\"r.msno\").alias(\"msno\"), F.col(\"r.snapshot_date\").alias(\"snapshot_date\"))\n",
    "    .agg(\n",
    "        F.sum(F.when(F.col(\"t.is_auto_renew\")==1, 1).otherwise(0)).alias(\"auto_renew_count\"),\n",
    "        F.count(F.lit(1)).alias(\"total_tx_before_expire\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"auto_renew_share\",\n",
    "        F.col(\"auto_renew_count\") / F.when(F.col(\"total_tx_before_expire\") > 0, F.col(\"total_tx_before_expire\")).otherwise(F.lit(1))\n",
    "    )\n",
    "    .select(\"msno\",\"snapshot_date\",\"auto_renew_share\")\n",
    ")\n",
    "\n",
    "# ========= Merge back into registered_users (per msno, snapshot_date)\n",
    "registered_users = (\n",
    "    registered_users\n",
    "    .join(tenure_asof.select(\"msno\",\"snapshot_date\",\"tenure_days\",\"last_is_auto_renew\",\"last_plan_list_price\"),\n",
    "          [\"msno\",\"snapshot_date\"], \"left\")\n",
    "    .join(auto_renew_stats, [\"msno\",\"snapshot_date\"], \"left\")\n",
    "    .na.fill({\n",
    "        \"tenure_days\": 0,\n",
    "        \"last_is_auto_renew\": 0,       # or leave null if you prefer\n",
    "        \"last_plan_list_price\": 0.0,   # or leave null if you prefer\n",
    "        \"auto_renew_share\": 0.0\n",
    "    })\n",
    ")\n",
    "\n",
    "cols_to_drop = [\"num_25_w30_sum\", \"num_50_w30_sum\", \"num_75_w30_sum\", \"num_985_w30_sum\", \"num_100_w30_sum\"]\n",
    "registered_users = registered_users.drop(*cols_to_drop)\n",
    "\n",
    "# Preview\n",
    "registered_users.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eabac2-c29a-4480-a91e-634457594beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582520c-aeb1-4ab3-a5a4-b817e98668ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# üß± Create Gold Features\n",
    "# ===============================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üóìÔ∏è Creating Gold Feature Store snapshot for all days\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "output_path = f\"datamart/gold/feature_store\"\n",
    "\n",
    "registered_users.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"‚úÖ Features saved to {output_path}\")\n",
    "print(f\"üìä Total records: {registered_users.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24aba4-926e-4b70-93b2-243c2c6bd273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
