{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ace8a3-865d-4c19-842b-fc97cab14af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee3a445-5236-44a7-aaa9-93cdbcccca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/31 07:30:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"dev\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9188f32-2384-423d-828b-a012f2a2e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=>(31 + 3) / 34][Stage 44:=> (6 + 7) / 13][Stage 46:>  (0 + 2) / 13]2 + 2) / 34][Stage 44:=> (6 + 7) / 13][Stage 46:>  (0 + 3) / 13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ Creating features for 2016-12-01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature store snapshot saved to: datamart/gold/feature_store/2016-12-01\n",
      "🏗️ Creating features for 2017-01-01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature store snapshot saved to: datamart/gold/feature_store/2017-01-01\n",
      "🏗️ Creating features for 2017-02-01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature store snapshot saved to: datamart/gold/feature_store/2017-02-01\n",
      "🏗️ Creating features for 2017-03-01 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature store snapshot saved to: datamart/gold/feature_store/2017-03-01\n",
      "✅ All monthly feature stores created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=>(31 + 3) / 34][Stage 44:=> (6 + 7) / 13][Stage 46:>  (0 + 2) / 13]"
     ]
    }
   ],
   "source": [
    "from utils.gold_feature_store import create_gold_features\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "start_date = datetime(2016, 12, 1)\n",
    "end_date   = datetime(2017, 3, 1)\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    inference_date = current.strftime(\"%Y-%m-%d\")\n",
    "    print(f\"🏗️ Creating features for {inference_date} ...\")\n",
    "    create_gold_features(inference_date, spark)\n",
    "    current += relativedelta(months=1)\n",
    "\n",
    "print(\"✅ All monthly feature stores created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cfdf7a-7b57-47fd-a37f-c666941bdc93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
