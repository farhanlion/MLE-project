# Section 1: Directory Tree ğŸ“‚
```
MLE-project/
â”‚
â”œâ”€â”€ data/                       # Raw KKBox CSVs
â”‚   â”œâ”€â”€ members_v3.csv
â”‚   â”œâ”€â”€ transactions*.csv
â”‚   â””â”€â”€ user_logs*.csv
â”‚
â”œâ”€â”€ datamart/                   # Medallion data warehouse
â”‚   â”œâ”€â”€ bronze/ (raw parquet, partitioned by year)
â”‚   â”œâ”€â”€ silver/ (cleaned + validated)
â”‚   â””â”€â”€ gold/   (feature_store/, label_store/)
â”‚
â”œâ”€â”€ airflow/                    # Airflow setup + config
â”œâ”€â”€ dags/                       # DAGs: data pipeline, training, monitoring
â”‚
â”œâ”€â”€ docs/                       # Project docs + diagrams
â”‚
â”œâ”€â”€ notebooks/                  # Dev notebooks
â”‚   â”œâ”€â”€ 01_eda/
â”‚   â”œâ”€â”€ 02_bronze_processing/
â”‚   â”œâ”€â”€ 03_silver_processing/
â”‚   â”œâ”€â”€ 04_gold_processing/
â”‚   â”œâ”€â”€ 05_model_training/
â”‚   â””â”€â”€ 06_model_inferencing/
â”‚
â”œâ”€â”€ scripts/                    # PySpark + ML training scripts
â”‚   â”œâ”€â”€ 01_bronze_*.py
â”‚   â”œâ”€â”€ 02_silver_*.py
â”‚   â”œâ”€â”€ 03_gold_*.py
â”‚   â”œâ”€â”€ 04_model_training_*.py
â”‚   â””â”€â”€ 05_model_inference_*.py
â”‚
â”œâ”€â”€ mlflow/                     # MLflow tracking/experiments
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ model_preprocessor.py
â”‚
â”œâ”€â”€ docker-compose.yaml         # Airflow + MLflow orchestration
â”‚
â”œâ”€â”€ 01_generate_medallion_tables.py     # Bronze/Silver/Gold pipeline (ETL)
â”œâ”€â”€ 02_main_training_pipeline.py        # Training + MLflow registration
â”œâ”€â”€ 03_inference_and_monitoring.py      # Batch/online inference + monitoring
â”‚
â””â”€â”€ README.md

```
# Section 2: How to Run 
## 1ï¸âƒ£ Start Environment

Make sure you have Docker + Docker Compose installed.  
Build and start all services (Airflow, MLflow, JupyterLab):
```bash
docker-compose up --build
```
Once started:  
| Service                | URL                                            |
| ---------------------- | ---------------------------------------------- |
| **Airflow Web UI**     | [http://localhost:8080](http://localhost:8080) |
| **MLflow Tracking UI** | [http://localhost:5000](http://localhost:5000) |
| **JupyterLab**         | [http://localhost:8888](http://localhost:8888) |


## 2ï¸âƒ£ Run Data Pipeline

### Option A â€“ via Airflow (Recommended)
Airflow DAGs are located in /dags:  
| DAG                                 | Purpose                                   |
| ----------------------------------- | ----------------------------------------- |
| `data_pipeline_dag.py`              | ETL pipeline (Bronze â†’ Silver â†’ Gold)     |
| `scheduled_training_dag.py`         | Scheduled model training & MLflow logging |
| `daily_inference_monitoring_dag.py` | Daily inference + model monitoring        |
  

Steps:  
	1.	Open Airflow UI (http://localhost:8080)  
	2.	Trigger the DAG manually or let it run on schedule  

### Option B â€“ via Python Scripts
Run specific stages manually:

Or run each script: 


```bash
python 01_generate_medallion_tables.py
```
Creates Bronze â†’ Silver â†’ Gold tables (full ETL)
```bash
python 02_main_training_pipeline.py 2016-04-02
```
Trains the model for a chosen date and logs results to MLflow.
```bash
python 03_inference_and_monitoring.py
``` 
Runs inference and performs drift + performance monitoring.
For simplicity and ease of debugging, we have hardcoded some of the dates and some of the dates are in variables. Which ofcourse will not be the case in a real deployment :)


