{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KKBox Churn Prediction - Model Training with MLflow\n",
    "\n",
    "This notebook trains three models (Logistic Regression, XGBoost, Random Forest) with hyperparameter tuning and comprehensive MLflow tracking.\n",
    "\n",
    "**Key Features:**\n",
    "- Separate encoding strategies for different model types\n",
    "- RandomizedSearchCV for efficient hyperparameter tuning\n",
    "- Class weight handling for imbalanced data\n",
    "- Nested MLflow runs (parent per model, child per CV fold)\n",
    "- Multiple evaluation metrics (ROC-AUC, Precision, Recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from scipy.stats import uniform, randint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking URI: http://mlflow:5000\n",
      "MLflow Experiment: 2\n"
     ]
    }
   ],
   "source": [
    "# MLflow configuration\n",
    "mlflow_tracking_uri = 'http://mlflow:5000'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(\"kkbox-churn-prediction\")\n",
    "\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow Experiment: {mlflow.get_experiment_by_name('kkbox-churn-prediction').experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_end_date': datetime.datetime(2016, 3, 31, 0, 0),\n",
      " 'data_start_date': datetime.datetime(2015, 2, 1, 0, 0),\n",
      " 'model_train_date': datetime.datetime(2016, 4, 1, 0, 0),\n",
      " 'model_train_date_str': '2016-04-01',\n",
      " 'oot_end_date': datetime.datetime(2016, 3, 31, 0, 0),\n",
      " 'oot_start_date': datetime.datetime(2016, 2, 1, 0, 0),\n",
      " 'test_end_date': datetime.datetime(2016, 1, 31, 0, 0),\n",
      " 'test_start_date': datetime.datetime(2015, 12, 1, 0, 0),\n",
      " 'train_end_date': datetime.datetime(2015, 9, 30, 0, 0),\n",
      " 'train_start_date': datetime.datetime(2015, 2, 1, 0, 0),\n",
      " 'val_end_date': datetime.datetime(2015, 11, 30, 0, 0),\n",
      " 'val_start_date': datetime.datetime(2015, 10, 1, 0, 0)}\n"
     ]
    }
   ],
   "source": [
    "## Set up config (using your 4-split approach)\n",
    "model_train_date_str = \"2016-04-01\"\n",
    "train_period_months = 8\n",
    "val_period_months = 2\n",
    "test_period_months = 2\n",
    "oot_period_months = 2\n",
    "\n",
    "config = {}\n",
    "config[\"model_train_date_str\"] = model_train_date_str\n",
    "config[\"model_train_date\"] = datetime.strptime(model_train_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "# Work backwards from model_train_date\n",
    "# OOT: Most recent data before deployment (2016-02-01 to 2016-03-31)\n",
    "config[\"oot_end_date\"] = config['model_train_date'] - timedelta(days=1)\n",
    "config[\"oot_start_date\"] = config['model_train_date'] - relativedelta(months=oot_period_months)\n",
    "\n",
    "# Test: Before OOT (2015-12-01 to 2016-01-31)\n",
    "config[\"test_end_date\"] = config[\"oot_start_date\"] - timedelta(days=1)\n",
    "config[\"test_start_date\"] = config[\"oot_start_date\"] - relativedelta(months=test_period_months)\n",
    "\n",
    "# Validation: Before Test (2015-10-01 to 2015-11-30)\n",
    "config[\"val_end_date\"] = config[\"test_start_date\"] - timedelta(days=1)\n",
    "config[\"val_start_date\"] = config[\"test_start_date\"] - relativedelta(months=val_period_months)\n",
    "\n",
    "# Training: Before Validation (2015-02-01 to 2015-09-30)\n",
    "config[\"train_end_date\"] = config[\"val_start_date\"] - timedelta(days=1)\n",
    "config[\"train_start_date\"] = config[\"val_start_date\"] - relativedelta(months=train_period_months)\n",
    "\n",
    "# NEW: Overall date range for extraction (covers all splits)\n",
    "config[\"data_start_date\"] = config[\"train_start_date\"]  # Earliest date needed\n",
    "config[\"data_end_date\"] = config[\"oot_end_date\"]        # Latest date needed\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Data & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pdf = pd.read_csv(\"data_pdf.csv\", parse_dates=[\"snapshot_date\"])\n",
    "data_pdf[\"snapshot_date\"] = data_pdf[\"snapshot_date\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Split Summary ===\n",
      "Training:   2015-02-01 to 2015-09-30 | 1,430,517 rows\n",
      "Validation: 2015-10-01 to 2015-11-30 | 447,007 rows\n",
      "Test:       2015-12-01 to 2016-01-31 | 517,744 rows\n",
      "OOT:        2016-02-01 to 2016-03-31 | 511,397 rows\n",
      "\n",
      "=== Churn Rates ===\n",
      "Training:   13.03%\n",
      "Validation: 12.12%\n",
      "Test:       20.12%\n",
      "OOT:        18.48%\n"
     ]
    }
   ],
   "source": [
    "# TIME-BASED SPLIT (NO SHUFFLING!) -- to prevent data leakages\n",
    "feature_cols = ['tenure_days_at_snapshot',\n",
    "                'registered_via',\n",
    "                'city_clean', \n",
    "                'sum_secs_w30',\n",
    "                'active_days_w30',\n",
    "                'complete_rate_w30',\n",
    "                'sum_secs_w7',\n",
    "                'engagement_ratio_7_30',\n",
    "                'days_since_last_play',\n",
    "                'trend_secs_w30',\n",
    "                'auto_renew_share',\n",
    "                'last_is_auto_renew']\n",
    "\n",
    "# Split data by time periods\n",
    "train_pdf = data_pdf[\n",
    "    (data_pdf['snapshot_date'] >= config[\"train_start_date\"].date()) & \n",
    "    (data_pdf['snapshot_date'] <= config[\"train_end_date\"].date())\n",
    "]\n",
    "\n",
    "val_pdf = data_pdf[\n",
    "    (data_pdf['snapshot_date'] >= config[\"val_start_date\"].date()) & \n",
    "    (data_pdf['snapshot_date'] <= config[\"val_end_date\"].date())\n",
    "]\n",
    "\n",
    "test_pdf = data_pdf[\n",
    "    (data_pdf['snapshot_date'] >= config[\"test_start_date\"].date()) & \n",
    "    (data_pdf['snapshot_date'] <= config[\"test_end_date\"].date())\n",
    "]\n",
    "\n",
    "oot_pdf = data_pdf[\n",
    "    (data_pdf['snapshot_date'] >= config[\"oot_start_date\"].date()) & \n",
    "    (data_pdf['snapshot_date'] <= config[\"oot_end_date\"].date())\n",
    "]\n",
    "\n",
    "# Create X, y splits\n",
    "X_train = train_pdf[feature_cols]\n",
    "y_train = train_pdf[\"label\"]\n",
    "\n",
    "X_val = val_pdf[feature_cols]\n",
    "y_val = val_pdf[\"label\"]\n",
    "\n",
    "X_test = test_pdf[feature_cols]\n",
    "y_test = test_pdf[\"label\"]\n",
    "\n",
    "X_oot = oot_pdf[feature_cols]\n",
    "y_oot = oot_pdf[\"label\"]\n",
    "\n",
    "# Verify splits\n",
    "print(\"\\n=== Data Split Summary ===\")\n",
    "print(f\"Training:   {config['train_start_date'].date()} to {config['train_end_date'].date()} | {len(train_pdf):,} rows\")\n",
    "print(f\"Validation: {config['val_start_date'].date()} to {config['val_end_date'].date()} | {len(val_pdf):,} rows\")\n",
    "print(f\"Test:       {config['test_start_date'].date()} to {config['test_end_date'].date()} | {len(test_pdf):,} rows\")\n",
    "print(f\"OOT:        {config['oot_start_date'].date()} to {config['oot_end_date'].date()} | {len(oot_pdf):,} rows\")\n",
    "\n",
    "print(\"\\n=== Churn Rates ===\")\n",
    "print(f\"Training:   {y_train.mean():.2%}\")\n",
    "print(f\"Validation: {y_val.mean():.2%}\")\n",
    "print(f\"Test:       {y_test.mean():.2%}\")\n",
    "print(f\"OOT:        {y_oot.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1430517, 12)\n",
      "Validation set shape: (447007, 12)\n",
      "Test set shape: (517744, 12)\n",
      "OOT set shape: (511397, 12)\n",
      "\n",
      "Class distribution in training set:\n",
      "label\n",
      "0    0.869677\n",
      "1    0.130323\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['tenure_days_at_snapshot', 'registered_via', 'city_clean', \n",
    "                'sum_secs_w30', 'active_days_w30', 'complete_rate_w30', \n",
    "                'sum_secs_w7', 'engagement_ratio_7_30', 'days_since_last_play', \n",
    "                'trend_secs_w30', 'auto_renew_share', 'last_is_auto_renew']\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"OOT set shape: {X_oot.shape}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Handle Missing Values & Encoding\n",
    "\n",
    "**Strategy**:\n",
    "1. Create missing value indicators\n",
    "2. Fill missing values with 0\n",
    "3. One-hot encode for Logistic Regression (with scaling)\n",
    "4. Keep original encoding for tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['registered_via', 'city_clean']\n",
      "Numerical columns: ['tenure_days_at_snapshot', 'sum_secs_w30', 'active_days_w30', 'complete_rate_w30', 'sum_secs_w7', 'engagement_ratio_7_30', 'days_since_last_play', 'trend_secs_w30', 'auto_renew_share', 'last_is_auto_renew']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['registered_via', 'city_clean']\n",
    "numerical_cols = [col for col in feature_cols if col not in categorical_cols]\n",
    "\n",
    "# Define feature groups for missing indicators\n",
    "activity_features = ['sum_secs_w30', 'active_days_w30', 'complete_rate_w30', \n",
    "                     'sum_secs_w7', 'engagement_ratio_7_30', 'days_since_last_play', \n",
    "                     'trend_secs_w30']\n",
    "demo_features = ['tenure_days_at_snapshot', 'registered_via', 'city_clean']\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Missing Value Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1] Creating missing value indicator features...\n",
      "  ‚úì Created 'is_missing_activity' indicator\n",
      "  ‚úì Created 'is_missing_demo' indicator\n",
      "  Train - Missing activity: 268,550 (18.8%)\n",
      "  Train - Missing demo: 140,268 (9.8%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 1] Creating missing value indicator features...\")\n",
    "\n",
    "# Create indicator features for ALL splits\n",
    "for df in [X_train, X_val, X_test, X_oot]:\n",
    "    # Indicator for missing activity features\n",
    "    df['is_missing_activity'] = df['sum_secs_w30'].isnull().astype(int)\n",
    "    \n",
    "    # Indicator for missing demographic features\n",
    "    df['is_missing_demo'] = df['tenure_days_at_snapshot'].isnull().astype(int)\n",
    "\n",
    "print(f\"  ‚úì Created 'is_missing_activity' indicator\")\n",
    "print(f\"  ‚úì Created 'is_missing_demo' indicator\")\n",
    "print(f\"  Train - Missing activity: {X_train['is_missing_activity'].sum():,} ({X_train['is_missing_activity'].mean():.1%})\")\n",
    "print(f\"  Train - Missing demo: {X_train['is_missing_demo'].sum():,} ({X_train['is_missing_demo'].mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fill Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Filling missing values with 0...\n",
      "  ‚úì All missing values filled with 0\n",
      "  Train missing values: 0\n",
      "  Val missing values: 0\n",
      "  Test missing values: 0\n",
      "  OOT missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 2] Filling missing values with 0...\")\n",
    "\n",
    "# Fill missing values with 0 (after creating indicators)\n",
    "for df in [X_train, X_val, X_test, X_oot]:\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "print(\"  ‚úì All missing values filled with 0\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"  Train missing values: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"  Val missing values: {X_val.isnull().sum().sum()}\")\n",
    "print(f\"  Test missing values: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"  OOT missing values: {X_oot.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Data for Logistic Regression (One-Hot Encoding + Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] One-hot encoding categorical features for Logistic Regression...\n",
      "  ‚úì One-hot encoded 'registered_via' and 'city_clean'\n",
      "  ‚úì Total features after encoding: 37\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 3] One-hot encoding categorical features for Logistic Regression...\")\n",
    "\n",
    "# Get dummies for registered_via and city_clean\n",
    "X_train_lr = pd.get_dummies(X_train, columns=['registered_via', 'city_clean'], \n",
    "                             drop_first=True, dtype=int)\n",
    "X_val_lr = pd.get_dummies(X_val, columns=['registered_via', 'city_clean'], \n",
    "                           drop_first=True, dtype=int)\n",
    "X_test_lr = pd.get_dummies(X_test, columns=['registered_via', 'city_clean'], \n",
    "                            drop_first=True, dtype=int)\n",
    "X_oot_lr = pd.get_dummies(X_oot, columns=['registered_via', 'city_clean'], \n",
    "                           drop_first=True, dtype=int)\n",
    "\n",
    "# Align columns across all datasets (handle unseen categories)\n",
    "all_columns = X_train_lr.columns\n",
    "for df in [X_val_lr, X_test_lr, X_oot_lr]:\n",
    "    # Add missing columns\n",
    "    for col in all_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "# Reassign to ensure column alignment\n",
    "X_val_lr = X_val_lr[all_columns]\n",
    "X_test_lr = X_test_lr[all_columns]\n",
    "X_oot_lr = X_oot_lr[all_columns]\n",
    "\n",
    "print(f\"  ‚úì One-hot encoded 'registered_via' and 'city_clean'\")\n",
    "print(f\"  ‚úì Total features after encoding: {X_train_lr.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Feature Scaling for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Scaling numeric features with StandardScaler...\n",
      "  Scaling 9 numeric features: ['tenure_days_at_snapshot', 'sum_secs_w30', 'active_days_w30', 'complete_rate_w30', 'sum_secs_w7', 'engagement_ratio_7_30', 'days_since_last_play', 'trend_secs_w30', 'auto_renew_share']\n",
      "  ‚úì Features scaled (mean=0, std=1)\n",
      "  ‚úì Logistic Regression data ready: (1430517, 37)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 4] Scaling numeric features with StandardScaler...\")\n",
    "\n",
    "# Identify numeric columns (exclude one-hot encoded columns and binary indicators)\n",
    "numeric_cols = [col for col in X_train_lr.columns \n",
    "                if not col.startswith('registered_via_') \n",
    "                and not col.startswith('city_clean_')\n",
    "                and col not in ['is_missing_activity', 'is_missing_demo', 'last_is_auto_renew']]\n",
    "\n",
    "print(f\"  Scaling {len(numeric_cols)} numeric features: {numeric_cols}\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only\n",
    "X_train_lr[numeric_cols] = scaler.fit_transform(X_train_lr[numeric_cols])\n",
    "\n",
    "# Transform validation, test, and OOT using the same scaler\n",
    "X_val_lr[numeric_cols] = scaler.transform(X_val_lr[numeric_cols])\n",
    "X_test_lr[numeric_cols] = scaler.transform(X_test_lr[numeric_cols])\n",
    "X_oot_lr[numeric_cols] = scaler.transform(X_oot_lr[numeric_cols])\n",
    "\n",
    "print(\"  ‚úì Features scaled (mean=0, std=1)\")\n",
    "print(f\"  ‚úì Logistic Regression data ready: {X_train_lr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Prepare Data for Tree-Based Models (Original Encoding, No Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Preparing data for tree-based models (XGBoost, Random Forest)...\n",
      "  ‚úì Tree-based model data ready: (1430517, 14)\n",
      "  ‚úì No scaling applied (tree models don't need it)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[STEP 5] Preparing data for tree-based models (XGBoost, Random Forest)...\")\n",
    "\n",
    "# For tree-based models, use the data after missing value handling but before one-hot encoding\n",
    "# Tree models can handle label-encoded categoricals and don't need scaling\n",
    "X_train_tree = X_train.copy()\n",
    "X_val_tree = X_val.copy()\n",
    "X_test_tree = X_test.copy()\n",
    "X_oot_tree = X_oot.copy()\n",
    "\n",
    "# Ensure categorical columns are integer type (safe now after fillna)\n",
    "for col in categorical_cols:\n",
    "    X_train_tree[col] = X_train_tree[col].astype(int)\n",
    "    X_val_tree[col] = X_val_tree[col].astype(int)\n",
    "    X_test_tree[col] = X_test_tree[col].astype(int)\n",
    "    X_oot_tree[col] = X_oot_tree[col].astype(int)\n",
    "\n",
    "print(f\"  ‚úì Tree-based model data ready: {X_train_tree.shape}\")\n",
    "print(f\"  ‚úì No scaling applied (tree models don't need it)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Evaluation and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, dataset_name=\"\", threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X: Feature matrix\n",
    "        y: True labels\n",
    "        dataset_name: Name of dataset (train/val/test/oot)\n",
    "        threshold: Classification threshold for precision/recall/F1\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        f'{dataset_name}_roc_auc': roc_auc_score(y, y_pred_proba),\n",
    "        f'{dataset_name}_precision': precision_score(y, y_pred, zero_division=0),\n",
    "        f'{dataset_name}_recall': recall_score(y, y_pred, zero_division=0),\n",
    "        f'{dataset_name}_f1': f1_score(y, y_pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred_proba, y_pred\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba, title=\"ROC Curve\"):\n",
    "    \"\"\"\n",
    "    Plot and return ROC curve figure.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot and return confusion matrix figure.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels(['No Churn', 'Churn'])\n",
    "    ax.set_yticklabels(['No Churn', 'Churn'])\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter Search Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning configuration:\n",
      "  - CV folds: 5\n",
      "  - Random search iterations: 50\n",
      "  - Scoring metric: roc_auc\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression hyperparameter space\n",
    "lr_param_dist = {\n",
    "    'C': uniform(0.01, 10),  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [1000, 2000, 3000]\n",
    "}\n",
    "\n",
    "# XGBoost hyperparameter space\n",
    "xgb_param_dist = {\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1)\n",
    "}\n",
    "\n",
    "# Random Forest hyperparameter space\n",
    "rf_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': [None] + list(range(5, 30, 5)),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Number of random parameter combinations to try\n",
    "N_ITER = 50  # Adjust based on computational budget\n",
    "\n",
    "print(f\"Hyperparameter tuning configuration:\")\n",
    "print(f\"  - CV folds: {cv.n_splits}\")\n",
    "print(f\"  - Random search iterations: {N_ITER}\")\n",
    "print(f\"  - Scoring metric: roc_auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Pipeline\n",
    "\n",
    "We'll create a reusable function that:\n",
    "1. Performs RandomizedSearchCV with nested MLflow logging\n",
    "2. Logs all hyperparameter combinations tried\n",
    "3. Evaluates on all datasets (train, val, test, oot)\n",
    "4. Creates and logs visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model_name, base_model, param_distributions, \n",
    "                        X_train, y_train, X_val, y_val, X_test, y_test, X_oot, y_oot,\n",
    "                        n_iter=50, cv=5):\n",
    "    \"\"\"\n",
    "    Train model with hyperparameter tuning and comprehensive MLflow logging.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model for MLflow tracking\n",
    "        base_model: Scikit-learn estimator\n",
    "        param_distributions: Dictionary of hyperparameter distributions\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        X_test, y_test: Test data\n",
    "        X_oot, y_oot: Out-of-time data\n",
    "        n_iter: Number of random search iterations\n",
    "        cv: Cross-validation strategy\n",
    "    \n",
    "    Returns:\n",
    "        best_model: The best model from RandomizedSearchCV\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Start parent run\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_hyperparameter_tuning\"):\n",
    "        \n",
    "        # Log parent run metadata\n",
    "        mlflow.set_tag(\"model_type\", model_name)\n",
    "        mlflow.set_tag(\"tuning_method\", \"RandomizedSearchCV\")\n",
    "        mlflow.log_param(\"n_iter\", n_iter)\n",
    "        mlflow.log_param(\"cv_folds\", cv.n_splits if hasattr(cv, 'n_splits') else cv)\n",
    "        mlflow.log_param(\"random_state\", RANDOM_STATE)\n",
    "        mlflow.log_param(\"train_samples\", len(y_train))\n",
    "        mlflow.log_param(\"val_samples\", len(y_val))\n",
    "        mlflow.log_param(\"test_samples\", len(y_test))\n",
    "        mlflow.log_param(\"oot_samples\", len(y_oot))\n",
    "        \n",
    "        # Perform RandomizedSearchCV\n",
    "        print(f\"\\n[INFO] Starting RandomizedSearchCV with {n_iter} iterations...\")\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"\\n[INFO] Best CV ROC-AUC: {random_search.best_score_:.4f}\")\n",
    "        print(f\"[INFO] Best parameters: {random_search.best_params_}\")\n",
    "        \n",
    "        # Log best parameters\n",
    "        for param_name, param_value in random_search.best_params_.items():\n",
    "            mlflow.log_param(f\"best_{param_name}\", param_value)\n",
    "        \n",
    "        mlflow.log_metric(\"best_cv_roc_auc\", random_search.best_score_)\n",
    "        \n",
    "        # Log all CV results as child runs\n",
    "        print(f\"\\n[INFO] Logging individual hyperparameter combinations...\")\n",
    "        cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "        \n",
    "        for idx in range(min(10, len(cv_results))):  # Log top 10 combinations\n",
    "            with mlflow.start_run(run_name=f\"{model_name}_trial_{idx+1}\", nested=True):\n",
    "                # Log parameters for this trial\n",
    "                params = cv_results.loc[idx, 'params']\n",
    "                for param_name, param_value in params.items():\n",
    "                    mlflow.log_param(param_name, param_value)\n",
    "                \n",
    "                # Log CV metrics\n",
    "                mlflow.log_metric(\"mean_cv_roc_auc\", cv_results.loc[idx, 'mean_test_score'])\n",
    "                mlflow.log_metric(\"std_cv_roc_auc\", cv_results.loc[idx, 'std_test_score'])\n",
    "                mlflow.log_metric(\"mean_train_roc_auc\", cv_results.loc[idx, 'mean_train_score'])\n",
    "                mlflow.log_metric(\"rank\", cv_results.loc[idx, 'rank_test_score'])\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = random_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on all datasets\n",
    "        print(f\"\\n[INFO] Evaluating best model on all datasets...\")\n",
    "        \n",
    "        # Training set\n",
    "        train_metrics, train_proba, train_pred = evaluate_model(\n",
    "            best_model, X_train, y_train, dataset_name=\"train\"\n",
    "        )\n",
    "        \n",
    "        # Validation set\n",
    "        val_metrics, val_proba, val_pred = evaluate_model(\n",
    "            best_model, X_val, y_val, dataset_name=\"val\"\n",
    "        )\n",
    "        \n",
    "        # Test set\n",
    "        test_metrics, test_proba, test_pred = evaluate_model(\n",
    "            best_model, X_test, y_test, dataset_name=\"test\"\n",
    "        )\n",
    "        \n",
    "        # OOT set\n",
    "        oot_metrics, oot_proba, oot_pred = evaluate_model(\n",
    "            best_model, X_oot, y_oot, dataset_name=\"oot\"\n",
    "        )\n",
    "        \n",
    "        # Log all metrics\n",
    "        all_metrics = {**train_metrics, **val_metrics, **test_metrics, **oot_metrics}\n",
    "        for metric_name, metric_value in all_metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n[RESULTS] {model_name} Performance:\")\n",
    "        print(f\"  Train ROC-AUC: {train_metrics['train_roc_auc']:.4f}\")\n",
    "        print(f\"  Val   ROC-AUC: {val_metrics['val_roc_auc']:.4f}\")\n",
    "        print(f\"  Test  ROC-AUC: {test_metrics['test_roc_auc']:.4f}\")\n",
    "        print(f\"  OOT   ROC-AUC: {oot_metrics['oot_roc_auc']:.4f}\")\n",
    "        \n",
    "        # Create and log visualizations\n",
    "        print(f\"\\n[INFO] Creating visualizations...\")\n",
    "        \n",
    "        # ROC curves for each dataset\n",
    "        for dataset_name, y_true, y_proba in [\n",
    "            ('train', y_train, train_proba),\n",
    "            ('val', y_val, val_proba),\n",
    "            ('test', y_test, test_proba),\n",
    "            ('oot', y_oot, oot_proba)\n",
    "        ]:\n",
    "            fig = plot_roc_curve(y_true, y_proba, title=f\"{model_name} - {dataset_name.upper()} ROC Curve\")\n",
    "            mlflow.log_figure(fig, f\"roc_curve_{dataset_name}.png\")\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # Confusion matrices for validation and test sets\n",
    "        for dataset_name, y_true, y_pred in [\n",
    "            ('val', y_val, val_pred),\n",
    "            ('test', y_test, test_pred),\n",
    "            ('oot', y_oot, oot_pred)\n",
    "        ]:\n",
    "            fig = plot_confusion_matrix(y_true, y_pred, \n",
    "                                       title=f\"{model_name} - {dataset_name.upper()} Confusion Matrix\")\n",
    "            mlflow.log_figure(fig, f\"confusion_matrix_{dataset_name}.png\")\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # Log model\n",
    "        print(f\"\\n[INFO] Logging model to MLflow...\")\n",
    "        mlflow.sklearn.log_model(best_model, f\"{model_name}_model\")\n",
    "        \n",
    "        # Log CV results dataframe\n",
    "        cv_results_path = f\"/tmp/{model_name}_cv_results.csv\"\n",
    "        cv_results.to_csv(cv_results_path, index=False)\n",
    "        mlflow.log_artifact(cv_results_path, \"cv_results\")\n",
    "        \n",
    "        print(f\"\\n[SUCCESS] {model_name} training complete!\")\n",
    "        \n",
    "    return best_model, all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Logistic Regression (with One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(0.5749259698670833), 1: np.float64(3.8366268123521556)}\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training LogisticRegression\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/04 10:44:46 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Starting RandomizedSearchCV with 50 iterations...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression with class weights\n",
    "lr_base = LogisticRegression(\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train with hyperparameter tuning\n",
    "lr_model, lr_metrics = train_and_log_model(\n",
    "    model_name=\"LogisticRegression\",\n",
    "    base_model=lr_base,\n",
    "    param_distributions=lr_param_dist,\n",
    "    X_train=X_train_lr,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_lr,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test_lr,\n",
    "    y_test=y_test,\n",
    "    X_oot=X_oot_lr,\n",
    "    y_oot=y_oot,\n",
    "    n_iter=N_ITER,\n",
    "    cv=cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train XGBoost (with Label Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for XGBoost (handles class imbalance)\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"XGBoost scale_pos_weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost\n",
    "xgb_base = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train with hyperparameter tuning\n",
    "xgb_model, xgb_metrics = train_and_log_model(\n",
    "    model_name=\"XGBoost\",\n",
    "    base_model=xgb_base,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    X_train=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_tree,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test_tree,\n",
    "    y_test=y_test,\n",
    "    X_oot=X_oot_tree,\n",
    "    y_oot=y_oot,\n",
    "    n_iter=N_ITER,\n",
    "    cv=cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Random Forest (with Label Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest with class weights\n",
    "rf_base = RandomForestClassifier(\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train with hyperparameter tuning\n",
    "rf_model, rf_metrics = train_and_log_model(\n",
    "    model_name=\"RandomForest\",\n",
    "    base_model=rf_base,\n",
    "    param_distributions=rf_param_dist,\n",
    "    X_train=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_tree,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test_tree,\n",
    "    y_test=y_test,\n",
    "    X_oot=X_oot_tree,\n",
    "    y_oot=y_oot,\n",
    "    n_iter=N_ITER,\n",
    "    cv=cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'XGBoost', 'Random Forest'],\n",
    "    'Train ROC-AUC': [\n",
    "        lr_metrics['train_roc_auc'],\n",
    "        xgb_metrics['train_roc_auc'],\n",
    "        rf_metrics['train_roc_auc']\n",
    "    ],\n",
    "    'Val ROC-AUC': [\n",
    "        lr_metrics['val_roc_auc'],\n",
    "        xgb_metrics['val_roc_auc'],\n",
    "        rf_metrics['val_roc_auc']\n",
    "    ],\n",
    "    'Test ROC-AUC': [\n",
    "        lr_metrics['test_roc_auc'],\n",
    "        xgb_metrics['test_roc_auc'],\n",
    "        rf_metrics['test_roc_auc']\n",
    "    ],\n",
    "    'OOT ROC-AUC': [\n",
    "        lr_metrics['oot_roc_auc'],\n",
    "        xgb_metrics['oot_roc_auc'],\n",
    "        rf_metrics['oot_roc_auc']\n",
    "    ],\n",
    "    'Test F1': [\n",
    "        lr_metrics['test_f1'],\n",
    "        xgb_metrics['test_f1'],\n",
    "        rf_metrics['test_f1']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best model\n",
    "best_model_idx = comparison_df['Test ROC-AUC'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nüèÜ Best Model (by Test ROC-AUC): {best_model_name}\")\n",
    "print(f\"   Test ROC-AUC: {comparison_df.loc[best_model_idx, 'Test ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# ROC-AUC comparison across datasets\n",
    "datasets = ['Train', 'Val', 'Test', 'OOT']\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.25\n",
    "\n",
    "axes[0].bar(x - width, comparison_df.iloc[:, 1:5].iloc[0], width, label='Logistic Regression')\n",
    "axes[0].bar(x, comparison_df.iloc[:, 1:5].iloc[1], width, label='XGBoost')\n",
    "axes[0].bar(x + width, comparison_df.iloc[:, 1:5].iloc[2], width, label='Random Forest')\n",
    "axes[0].set_xlabel('Dataset')\n",
    "axes[0].set_ylabel('ROC-AUC')\n",
    "axes[0].set_title('Model Comparison: ROC-AUC Across Datasets')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(datasets)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "\n",
    "# Test set metrics comparison\n",
    "test_metrics_data = {\n",
    "    'ROC-AUC': comparison_df['Test ROC-AUC'].values,\n",
    "    'F1': comparison_df['Test F1'].values\n",
    "}\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, test_metrics_data['ROC-AUC'], width, label='ROC-AUC')\n",
    "axes[1].bar(x + width/2, test_metrics_data['F1'], width, label='F1')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Test Set Metrics Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Log comparison to MLflow\n",
    "with mlflow.start_run(run_name=\"Model_Comparison_Summary\"):\n",
    "    mlflow.log_figure(fig, \"model_comparison.png\")\n",
    "    comparison_df.to_csv('/tmp/model_comparison.csv', index=False)\n",
    "    mlflow.log_artifact('/tmp/model_comparison.csv')\n",
    "    mlflow.log_metric(\"best_test_roc_auc\", comparison_df['Test ROC-AUC'].max())\n",
    "    mlflow.set_tag(\"best_model\", best_model_name)\n",
    "\n",
    "print(\"\\n‚úÖ Model comparison logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Review MLflow UI**: Open `http://mlflow:5000` to explore:\n",
    "   - All hyperparameter combinations tried\n",
    "   - Metrics across different datasets\n",
    "   - Model artifacts and visualizations\n",
    "\n",
    "2. **Model Selection**: Based on the comparison above, select your best model for production\n",
    "\n",
    "3. **Model Registry**: Register the best model in MLflow Model Registry:\n",
    "   ```python\n",
    "   # Example:\n",
    "   model_uri = f\"runs:/<run_id>/XGBoost_model\"\n",
    "   mlflow.register_model(model_uri, \"kkbox-churn-predictor\")\n",
    "   ```\n",
    "\n",
    "4. **Further Tuning**: If needed:\n",
    "   - Increase `N_ITER` for more thorough search\n",
    "   - Try different threshold values for precision/recall trade-off\n",
    "   - Perform feature engineering based on model insights\n",
    "   - Consider ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
