{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "Generate synthetic data and run XGBoost training pipeline (test).\n",
    "This mirrors the signatures used by your training script.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ==== Adjust these if you want a different test size ====\n",
    "N_SAMPLES = 5000\n",
    "N_FEATURES = 20\n",
    "N_INFORMATIVE = 8\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(\"fake-data-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking URI: http://mlflow:5000\n",
      "MLflow Experiment: 2\n"
     ]
    }
   ],
   "source": [
    "# MLflow configuration\n",
    "mlflow_tracking_uri = 'http://mlflow:5000'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(\"kkbox-churn-prediction-fake\")\n",
    "\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow Experiment: {mlflow.get_experiment_by_name('kkbox-churn-prediction-fake').experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_end_date': datetime.datetime(2016, 3, 31, 0, 0),\n",
      " 'data_start_date': datetime.datetime(2015, 2, 1, 0, 0),\n",
      " 'model_train_date': datetime.datetime(2016, 4, 1, 0, 0),\n",
      " 'model_train_date_str': '2016-04-01',\n",
      " 'oot_end_date': datetime.datetime(2016, 3, 31, 0, 0),\n",
      " 'oot_start_date': datetime.datetime(2016, 2, 1, 0, 0),\n",
      " 'test_end_date': datetime.datetime(2016, 1, 31, 0, 0),\n",
      " 'test_start_date': datetime.datetime(2015, 12, 1, 0, 0),\n",
      " 'train_end_date': datetime.datetime(2015, 9, 30, 0, 0),\n",
      " 'train_start_date': datetime.datetime(2015, 2, 1, 0, 0),\n",
      " 'val_end_date': datetime.datetime(2015, 11, 30, 0, 0),\n",
      " 'val_start_date': datetime.datetime(2015, 10, 1, 0, 0)}\n"
     ]
    }
   ],
   "source": [
    "## Set up config (using your 4-split approach)\n",
    "model_train_date_str = \"2016-04-01\"\n",
    "train_period_months = 8\n",
    "val_period_months = 2\n",
    "test_period_months = 2\n",
    "oot_period_months = 2\n",
    "\n",
    "config = {}\n",
    "config[\"model_train_date_str\"] = model_train_date_str\n",
    "config[\"model_train_date\"] = datetime.strptime(model_train_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "# Work backwards from model_train_date\n",
    "# OOT: Most recent data before deployment (2016-02-01 to 2016-03-31)\n",
    "config[\"oot_end_date\"] = config['model_train_date'] - timedelta(days=1)\n",
    "config[\"oot_start_date\"] = config['model_train_date'] - relativedelta(months=oot_period_months)\n",
    "\n",
    "# Test: Before OOT (2015-12-01 to 2016-01-31)\n",
    "config[\"test_end_date\"] = config[\"oot_start_date\"] - timedelta(days=1)\n",
    "config[\"test_start_date\"] = config[\"oot_start_date\"] - relativedelta(months=test_period_months)\n",
    "\n",
    "# Validation: Before Test (2015-10-01 to 2015-11-30)\n",
    "config[\"val_end_date\"] = config[\"test_start_date\"] - timedelta(days=1)\n",
    "config[\"val_start_date\"] = config[\"test_start_date\"] - relativedelta(months=val_period_months)\n",
    "\n",
    "# Training: Before Validation (2015-02-01 to 2015-09-30)\n",
    "config[\"train_end_date\"] = config[\"val_start_date\"] - timedelta(days=1)\n",
    "config[\"train_start_date\"] = config[\"val_start_date\"] - relativedelta(months=train_period_months)\n",
    "\n",
    "# NEW: Overall date range for extraction (covers all splits)\n",
    "config[\"data_start_date\"] = config[\"train_start_date\"]  # Earliest date needed\n",
    "config[\"data_end_date\"] = config[\"oot_end_date\"]        # Latest date needed\n",
    "\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the \"utils\" folder to PYTHONPATH (works in notebooks)\n",
    "sys.path.append(str(Path().resolve().parent.parent / \"utils\"))\n",
    "\n",
    "from model_preprocessor import prepare_data_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 09:13:13,643 INFO Config date windows computed\n",
      "2025-11-07 09:13:13,643 INFO Train: 2015-03-01 ‚Üí 2015-10-31\n",
      "2025-11-07 09:13:13,644 INFO Val:   2015-11-01 ‚Üí 2015-12-31\n",
      "2025-11-07 09:13:13,645 INFO Test:  2016-01-01 ‚Üí 2016-02-29\n",
      "2025-11-07 09:13:13,645 INFO OOT:   2016-03-01 ‚Üí 2016-04-30\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/07 09:13:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/07 09:13:27 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "2025-11-07 09:13:33,931 INFO Filtered features rows: 20158616                   \n",
      "2025-11-07 09:13:34,462 INFO Filtered labels rows: 10079308\n",
      "2025-11-07 09:13:53,832 INFO Joined rows: 20158616                              \n",
      "2025-11-07 09:13:53,969 WARNING Using fallback label column: is_churn\n",
      "2025-11-07 09:14:07,906 INFO train rows before sampling: 10154064               \n",
      "2025-11-07 09:14:19,847 INFO val rows before sampling: 3251302                  \n",
      "2025-11-07 09:14:32,251 INFO test rows before sampling: 3352918                 \n",
      "2025-11-07 09:14:45,292 INFO oot rows before sampling: 3400332                  \n",
      "2025-11-07 09:15:02,322 INFO Sampling fractions per label: {1: 0.3, 0: 0.3}     \n",
      "2025-11-07 09:15:19,502 INFO Sampled rows: 3048967                              \n",
      "2025-11-07 09:18:24,416 INFO Converted Spark DF to pandas: (3046492, 25)        \n",
      "2025-11-07 09:18:37,153 INFO Sampling fractions per label: {1: 0.3, 0: 0.3}     \n",
      "2025-11-07 09:18:48,924 INFO Sampled rows: 975699                               \n",
      "2025-11-07 09:19:47,382 INFO Converted Spark DF to pandas: (975699, 25)         \n",
      "2025-11-07 09:19:59,467 INFO Sampling fractions per label: {1: 0.3, 0: 0.3}     \n",
      "2025-11-07 09:20:09,709 INFO Sampled rows: 1006214                              \n",
      "2025-11-07 09:21:09,353 INFO Converted Spark DF to pandas: (1006214, 25)        \n",
      "2025-11-07 09:21:21,111 INFO Sampling fractions per label: {1: 0.3, 0: 0.3}     \n",
      "2025-11-07 09:21:31,066 INFO Sampled rows: 1020422                              \n",
      "2025-11-07 09:22:33,694 INFO Converted Spark DF to pandas: (1020422, 25)        \n",
      "2025-11-07 09:22:34,198 INFO Stopped Spark - continuing in pandas\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    train_date=\"2016-05-01\",\n",
    "    features_path=\"/app/datamart/gold/feature_store/\",\n",
    "    labels_path=\"/app/datamart/gold/label_store/\",\n",
    "    sample_frac=0.3,\n",
    "    label_col=\"label\",\n",
    "    mlflow_tracking_uri=\"http://mlflow:5000\",\n",
    "    mlflow_experiment=\"kkbox-churn-prediction\",\n",
    "    n_iter=10,\n",
    "    cv_folds=5,\n",
    "    random_state=42,\n",
    "    train_months=8,\n",
    "    val_months=2,\n",
    "    test_months=2,\n",
    "    oot_months=2\n",
    ")\n",
    "\n",
    "data = prepare_data_for_training(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: set()\n",
      "Extra keys: set()\n",
      "X_train_lr: type=<class 'pandas.core.frame.DataFrame'>, shape=(3046492, 37)\n",
      "X_val_lr: type=<class 'pandas.core.frame.DataFrame'>, shape=(975699, 37)\n",
      "X_test_lr: type=<class 'pandas.core.frame.DataFrame'>, shape=(1006214, 37)\n",
      "X_oot_lr: type=<class 'pandas.core.frame.DataFrame'>, shape=(1020422, 37)\n",
      "y_train: type=<class 'pandas.core.series.Series'>, length=3046492, dtype=int64\n",
      "y_val: type=<class 'pandas.core.series.Series'>, length=975699, dtype=int64\n",
      "y_test: type=<class 'pandas.core.series.Series'>, length=1006214, dtype=int64\n",
      "y_oot: type=<class 'pandas.core.series.Series'>, length=1020422, dtype=int64\n",
      "lr_cols len: 37\n",
      "categorical_cols: ['registered_via', 'city_clean']\n",
      "class_weight_dict: {0: np.float64(0.5770500721290808), 1: np.float64(3.7446432961305867)}\n",
      "scale_pos_weight: 6.489286592261173\n",
      "cv: StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
      "y_train value counts:\n",
      " {0: 2639712, 1: 406780}\n"
     ]
    }
   ],
   "source": [
    "# assume out = prepare_data_for_training(args)\n",
    "def inspect_prepared(out):\n",
    "    expected_keys = {\n",
    "        \"X_train_lr\",\"y_train\",\"X_val_lr\",\"y_val\",\"X_test_lr\",\"y_test\",\n",
    "        \"X_oot_lr\",\"y_oot\",\"X_train_tree\",\"X_val_tree\",\"X_test_tree\",\"X_oot_tree\",\n",
    "        \"scaler\",\"lr_cols\",\"categorical_cols\",\"class_weight_dict\",\"scale_pos_weight\",\"cv\"\n",
    "    }\n",
    "    missing = expected_keys - set(out.keys())\n",
    "    extra = set(out.keys()) - expected_keys\n",
    "\n",
    "    print(\"Missing keys:\", missing)\n",
    "    print(\"Extra keys:\", extra)\n",
    "    # quick shapes / types\n",
    "    for k in [\"X_train_lr\",\"X_val_lr\",\"X_test_lr\",\"X_oot_lr\"]:\n",
    "        v = out.get(k)\n",
    "        print(f\"{k}: type={type(v)}, shape={getattr(v,'shape',None)}\")\n",
    "    for k in [\"y_train\",\"y_val\",\"y_test\",\"y_oot\"]:\n",
    "        v = out.get(k)\n",
    "        print(f\"{k}: type={type(v)}, length={len(v) if v is not None else None}, dtype={getattr(v,'dtype',None)}\")\n",
    "    print(\"lr_cols len:\", len(out.get(\"lr_cols\", [])))\n",
    "    print(\"categorical_cols:\", out.get(\"categorical_cols\"))\n",
    "    print(\"class_weight_dict:\", out.get(\"class_weight_dict\"))\n",
    "    print(\"scale_pos_weight:\", out.get(\"scale_pos_weight\"))\n",
    "    print(\"cv:\", out.get(\"cv\"))\n",
    "    # spot-check class balance\n",
    "    if \"y_train\" in out:\n",
    "        print(\"y_train value counts:\\n\", out[\"y_train\"].value_counts(normalize=False).to_dict())\n",
    "\n",
    "# call:\n",
    "inspect_prepared(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight=data[\"class_weight_dict\"], random_state=args.random_state, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"X_train_lr\"] = data[\"X_train_lr\"].sample(frac=0.2, random_state=42)\n",
    "data[\"y_train\"] = data[\"y_train\"].loc[data[\"X_train_lr\"].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/07 09:50:21 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.80897\tval-auc:0.67438\n",
      "[10]\ttrain-auc:0.83227\tval-auc:0.68234\n",
      "[20]\ttrain-auc:0.84130\tval-auc:0.68151\n",
      "[30]\ttrain-auc:0.85199\tval-auc:0.68252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/07 09:50:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "/usr/local/lib/python3.12/site-packages/mlflow/xgboost/__init__.py:169: UserWarning: [09:50:36] WARNING: /workspace/src/c_api/c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  xgb_model.save_model(model_data_path)\n",
      "\u001b[31m2025/11/07 09:50:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow run id: 2ddc6524115b4d0ebd2d3161c7081289\n",
      "Test ROC-AUC: 0.742001436518704\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9375    0.6560    0.7719    853682\n",
      "           1     0.2818    0.7554    0.4104    152532\n",
      "\n",
      "    accuracy                         0.6710   1006214\n",
      "   macro avg     0.6097    0.7057    0.5912   1006214\n",
      "weighted avg     0.8381    0.6710    0.7171   1006214\n",
      "\n",
      "üèÉ View run xgb_churn at: http://mlflow:5000/#/experiments/2/runs/2ddc6524115b4d0ebd2d3161c7081289\n",
      "üß™ View experiment at: http://mlflow:5000/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# --- Safety checks & DMatrix creation (use correct keys)\n",
    "X_train = data[\"X_train_lr\"]\n",
    "y_train = data[\"y_train\"]        # <-- use this key (not y_train_lr unless you have it)\n",
    "X_val   = data[\"X_val_lr\"]\n",
    "y_val   = data[\"y_val\"]\n",
    "X_test  = data[\"X_test_lr\"]\n",
    "y_test  = data[\"y_test\"]\n",
    "\n",
    "# optional: if you downsampled earlier and indices are not reset, reset them now\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "assert len(X_train) == len(y_train), f\"train mismatch: {len(X_train)} vs {len(y_train)}\"\n",
    "assert len(X_val)   == len(y_val),   f\"val mismatch:   {len(X_val)}   vs {len(y_val)}\"\n",
    "assert len(X_test)  == len(y_test),  f\"test mismatch:  {len(X_test)}  vs {len(y_test)}\"\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train.values)\n",
    "dval   = xgb.DMatrix(X_val,   label=y_val.values)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test.values)\n",
    "\n",
    "# --- Params\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"scale_pos_weight\": data.get(\"scale_pos_weight\", 1.0),\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"seed\": 42\n",
    "}\n",
    "num_boost_round = 300\n",
    "early_stopping_rounds = 20\n",
    "\n",
    "# --- Directory to save local artifacts that we'll log to MLflow\n",
    "local_artifact_dir = \"mlflow_artifacts\"\n",
    "os.makedirs(local_artifact_dir, exist_ok=True)\n",
    "\n",
    "# Optionally set MLflow tracking URI if you use a remote server:\n",
    "# mlflow.set_tracking_uri(\"http://your-mlflow-server:5000\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgb_churn\") as run:\n",
    "    # log params\n",
    "    mlflow.log_params({**params, \"num_boost_round\": num_boost_round, \"early_stopping_rounds\": early_stopping_rounds})\n",
    "\n",
    "    # train\n",
    "    evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
    "    model = xgb.train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "\n",
    "    # log model with MLflow XGBoost flavor\n",
    "    mlflow.xgboost.log_model(model, artifact_path=\"xgb_model\")\n",
    "\n",
    "    # predictions + metrics\n",
    "    y_pred_proba = model.predict(dtest)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    auc = float(roc_auc_score(y_test, y_pred_proba))\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    mlflow.log_metric(\"test_auc\", auc)\n",
    "\n",
    "    # Save classification report text and log it\n",
    "    report_path = os.path.join(local_artifact_dir, \"classification_report.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "        f.write(\"\\nTest ROC-AUC: {:.6f}\\n\".format(auc))\n",
    "    mlflow.log_artifact(report_path)\n",
    "\n",
    "    # Save feature importance (as JSON) and log it\n",
    "    fmap = model.get_score(importance_type=\"weight\")  # {feature:score}\n",
    "    fi_path = os.path.join(local_artifact_dir, \"feature_importance.json\")\n",
    "    with open(fi_path, \"w\") as f:\n",
    "        json.dump(fmap, f, indent=2)\n",
    "    mlflow.log_artifact(fi_path)\n",
    "\n",
    "    # Bundle model + scaler + lr_cols into a single pickle and log as artifact\n",
    "    bundle = {\n",
    "        \"xgb_model\": model,               # Booster object\n",
    "        \"lr_cols\": data[\"lr_cols\"],\n",
    "        \"scaler\": data.get(\"scaler\", None),\n",
    "        \"threshold\": 0.5\n",
    "    }\n",
    "    pkl_path = os.path.join(local_artifact_dir, \"xgb_bundle.pkl\")\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(bundle, f)\n",
    "    mlflow.log_artifact(pkl_path)\n",
    "\n",
    "    # (Optional) also save the raw X_test head for quick debugging\n",
    "    X_test.head(1000).to_parquet(os.path.join(local_artifact_dir, \"X_test_head.parquet\"), index=False)\n",
    "    mlflow.log_artifact(os.path.join(local_artifact_dir, \"X_test_head.parquet\"))\n",
    "\n",
    "    # final logging of run id\n",
    "    run_id = run.info.run_id\n",
    "    print(\"MLflow run id:\", run_id)\n",
    "    print(\"Test ROC-AUC:\", auc)\n",
    "    print(\"Classification report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x77d679c76e40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
