{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KKBox Churn Prediction - Model Training with MLflow\n",
    "\n",
    "This notebook trains three models (Logistic Regression, XGBoost, Random Forest) with hyperparameter tuning and comprehensive MLflow tracking.\n",
    "\n",
    "**Key Features:**\n",
    "- Separate encoding strategies for different model types\n",
    "- RandomizedSearchCV for efficient hyperparameter tuning\n",
    "- Class weight handling for imbalanced data\n",
    "- Nested MLflow runs (parent per model, child per CV fold)\n",
    "- Multiple evaluation metrics (ROC-AUC, Precision, Recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from scipy.stats import uniform, randint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow configuration\n",
    "mlflow_tracking_uri = 'http://mlflow:5000'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(\"kkbox-churn-prediction\")\n",
    "\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow Experiment: {mlflow.get_experiment_by_name('kkbox-churn-prediction').experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Your Prepared Data\n",
    "\n",
    "**Note**: Replace this cell with your actual data loading code. \n",
    "The script expects these variables to be defined:\n",
    "- `X_train`, `y_train`\n",
    "- `X_val`, `y_val`\n",
    "- `X_test`, `y_test`\n",
    "- `X_oot`, `y_oot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pdf = pd.read_csv(\"data_pdf.csv\", parse_dates=[\"snapshot_date\"])\n",
    "data_pdf[\"snapshot_date\"] = data_pdf[\"snapshot_date\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR DATA LOADING CODE HERE\n",
    "# Example:\n",
    "# train_pdf = pd.read_csv('train_data.csv')\n",
    "# val_pdf = pd.read_csv('val_data.csv')\n",
    "# test_pdf = pd.read_csv('test_data.csv')\n",
    "# oot_pdf = pd.read_csv('oot_data.csv')\n",
    "\n",
    "feature_cols = ['tenure_days_at_snapshot', 'registered_via', 'city_clean', \n",
    "                'sum_secs_w30', 'active_days_w30', 'complete_rate_w30', \n",
    "                'sum_secs_w7', 'engagement_ratio_7_30', 'days_since_last_play', \n",
    "                'trend_secs_w30', 'auto_renew_share', 'last_is_auto_renew']\n",
    "\n",
    "# X_train = train_pdf[feature_cols]\n",
    "# y_train = train_pdf[\"label\"]\n",
    "# X_val = val_pdf[feature_cols]\n",
    "# y_val = val_pdf[\"label\"]\n",
    "# X_test = test_pdf[feature_cols]\n",
    "# y_test = test_pdf[\"label\"]\n",
    "# X_oot = oot_pdf[feature_cols]\n",
    "# y_oot = oot_pdf[\"label\"]\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"OOT set shape: {X_oot.shape}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Proper Encoding Strategy\n",
    "\n",
    "**Important**: We fit encoders on training data and transform all sets consistently to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['registered_via', 'city_clean']\n",
    "numerical_cols = [col for col in feature_cols if col not in categorical_cols]\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoder fitted on training data (for Logistic Regression)\n",
    "print(\"\\n[INFO] Creating one-hot encoder for Logistic Regression...\")\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "ohe.fit(X_train[categorical_cols])\n",
    "\n",
    "# Transform all datasets\n",
    "X_train_cat_ohe = ohe.transform(X_train[categorical_cols])\n",
    "X_val_cat_ohe = ohe.transform(X_val[categorical_cols])\n",
    "X_test_cat_ohe = ohe.transform(X_test[categorical_cols])\n",
    "X_oot_cat_ohe = ohe.transform(X_oot[categorical_cols])\n",
    "\n",
    "# Get feature names\n",
    "ohe_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Combine with numerical features\n",
    "X_train_lr = np.hstack([X_train[numerical_cols].values, X_train_cat_ohe])\n",
    "X_val_lr = np.hstack([X_val[numerical_cols].values, X_val_cat_ohe])\n",
    "X_test_lr = np.hstack([X_test[numerical_cols].values, X_test_cat_ohe])\n",
    "X_oot_lr = np.hstack([X_oot[numerical_cols].values, X_oot_cat_ohe])\n",
    "\n",
    "print(f\"One-hot encoded training shape: {X_train_lr.shape}\")\n",
    "print(f\"Number of one-hot encoded features: {len(ohe_feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tree-based models (XGBoost, Random Forest), use label encoding\n",
    "# Simply ensure categorical columns are integer type\n",
    "print(\"\\n[INFO] Preparing data for tree-based models (using original encoding)...\")\n",
    "\n",
    "X_train_tree = X_train.copy()\n",
    "X_val_tree = X_val.copy()\n",
    "X_test_tree = X_test.copy()\n",
    "X_oot_tree = X_oot.copy()\n",
    "\n",
    "# Ensure categorical columns are integer type\n",
    "for col in categorical_cols:\n",
    "    X_train_tree[col] = X_train_tree[col].astype(int)\n",
    "    X_val_tree[col] = X_val_tree[col].astype(int)\n",
    "    X_test_tree[col] = X_test_tree[col].astype(int)\n",
    "    X_oot_tree[col] = X_oot_tree[col].astype(int)\n",
    "\n",
    "print(f\"Tree-based model training shape: {X_train_tree.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Evaluation and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, dataset_name=\"\", threshold=0.5):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X: Feature matrix\n",
    "        y: True labels\n",
    "        dataset_name: Name of dataset (train/val/test/oot)\n",
    "        threshold: Classification threshold for precision/recall/F1\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        f'{dataset_name}_roc_auc': roc_auc_score(y, y_pred_proba),\n",
    "        f'{dataset_name}_precision': precision_score(y, y_pred, zero_division=0),\n",
    "        f'{dataset_name}_recall': recall_score(y, y_pred, zero_division=0),\n",
    "        f'{dataset_name}_f1': f1_score(y, y_pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred_proba, y_pred\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba, title=\"ROC Curve\"):\n",
    "    \"\"\"\n",
    "    Plot and return ROC curve figure.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot and return confusion matrix figure.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels(['No Churn', 'Churn'])\n",
    "    ax.set_yticklabels(['No Churn', 'Churn'])\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter Search Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression hyperparameter space\n",
    "lr_param_dist = {\n",
    "    'C': uniform(0.01, 10),  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [1000, 2000, 3000]\n",
    "}\n",
    "\n",
    "# XGBoost hyperparameter space\n",
    "xgb_param_dist = {\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 5),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1)\n",
    "}\n",
    "\n",
    "# Random Forest hyperparameter space\n",
    "rf_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': [None] + list(range(5, 30, 5)),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Number of random parameter combinations to try\n",
    "N_ITER = 50  # Adjust based on computational budget\n",
    "\n",
    "print(f\"Hyperparameter tuning configuration:\")\n",
    "print(f\"  - CV folds: {cv.n_splits}\")\n",
    "print(f\"  - Random search iterations: {N_ITER}\")\n",
    "print(f\"  - Scoring metric: roc_auc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Pipeline\n",
    "\n",
    "We'll create a reusable function that:\n",
    "1. Performs RandomizedSearchCV with nested MLflow logging\n",
    "2. Logs all hyperparameter combinations tried\n",
    "3. Evaluates on all datasets (train, val, test, oot)\n",
    "4. Creates and logs visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model_name, base_model, param_distributions, \n",
    "                        X_train, y_train, X_val, y_val, X_test, y_test, X_oot, y_oot,\n",
    "                        n_iter=50, cv=5):\n",
    "    \"\"\"\n",
    "    Train model with hyperparameter tuning and comprehensive MLflow logging.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model for MLflow tracking\n",
    "        base_model: Scikit-learn estimator\n",
    "        param_distributions: Dictionary of hyperparameter distributions\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        X_test, y_test: Test data\n",
    "        X_oot, y_oot: Out-of-time data\n",
    "        n_iter: Number of random search iterations\n",
    "        cv: Cross-validation strategy\n",
    "    \n",
    "    Returns:\n",
    "        best_model: The best model from RandomizedSearchCV\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Start parent run\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_hyperparameter_tuning\"):\n",
    "        \n",
    "        # Log parent run metadata\n",
    "        mlflow.set_tag(\"model_type\", model_name)\n",
    "        mlflow.set_tag(\"tuning_method\", \"RandomizedSearchCV\")\n",
    "        mlflow.log_param(\"n_iter\", n_iter)\n",
    "        mlflow.log_param(\"cv_folds\", cv.n_splits if hasattr(cv, 'n_splits') else cv)\n",
    "        mlflow.log_param(\"random_state\", RANDOM_STATE)\n",
    "        mlflow.log_param(\"train_samples\", len(y_train))\n",
    "        mlflow.log_param(\"val_samples\", len(y_val))\n",
    "        mlflow.log_param(\"test_samples\", len(y_test))\n",
    "        mlflow.log_param(\"oot_samples\", len(y_oot))\n",
    "        \n",
    "        # Perform RandomizedSearchCV\n",
    "        print(f\"\\n[INFO] Starting RandomizedSearchCV with {n_iter} iterations...\")\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=base_model,\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"\\n[INFO] Best CV ROC-AUC: {random_search.best_score_:.4f}\")\n",
    "        print(f\"[INFO] Best parameters: {random_search.best_params_}\")\n",
    "        \n",
    "        # Log best parameters\n",
    "        for param_name, param_value in random_search.best_params_.items():\n",
    "            mlflow.log_param(f\"best_{param_name}\", param_value)\n",
    "        \n",
    "        mlflow.log_metric(\"best_cv_roc_auc\", random_search.best_score_)\n",
    "        \n",
    "        # Log all CV results as child runs\n",
    "        print(f\"\\n[INFO] Logging individual hyperparameter combinations...\")\n",
    "        cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "        \n",
    "        for idx in range(min(10, len(cv_results))):  # Log top 10 combinations\n",
    "            with mlflow.start_run(run_name=f\"{model_name}_trial_{idx+1}\", nested=True):\n",
    "                # Log parameters for this trial\n",
    "                params = cv_results.loc[idx, 'params']\n",
    "                for param_name, param_value in params.items():\n",
    "                    mlflow.log_param(param_name, param_value)\n",
    "                \n",
    "                # Log CV metrics\n",
    "                mlflow.log_metric(\"mean_cv_roc_auc\", cv_results.loc[idx, 'mean_test_score'])\n",
    "                mlflow.log_metric(\"std_cv_roc_auc\", cv_results.loc[idx, 'std_test_score'])\n",
    "                mlflow.log_metric(\"mean_train_roc_auc\", cv_results.loc[idx, 'mean_train_score'])\n",
    "                mlflow.log_metric(\"rank\", cv_results.loc[idx, 'rank_test_score'])\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = random_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on all datasets\n",
    "        print(f\"\\n[INFO] Evaluating best model on all datasets...\")\n",
    "        \n",
    "        # Training set\n",
    "        train_metrics, train_proba, train_pred = evaluate_model(\n",
    "            best_model, X_train, y_train, dataset_name=\"train\"\n",
    "        )\n",
    "        \n",
    "        # Validation set\n",
    "        val_metrics, val_proba, val_pred = evaluate_model(\n",
    "            best_model, X_val, y_val, dataset_name=\"val\"\n",
    "        )\n",
    "        \n",
    "        # Test set\n",
    "        test_metrics, test_proba, test_pred = evaluate_model(\n",
    "            best_model, X_test, y_test, dataset_name=\"test\"\n",
    "        )\n",
    "        \n",
    "        # OOT set\n",
    "        oot_metrics, oot_proba, oot_pred = evaluate_model(\n",
    "            best_model, X_oot, y_oot, dataset_name=\"oot\"\n",
    "        )\n",
    "        \n",
    "        # Log all metrics\n",
    "        all_metrics = {**train_metrics, **val_metrics, **test_metrics, **oot_metrics}\n",
    "        for metric_name, metric_value in all_metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n[RESULTS] {model_name} Performance:\")\n",
    "        print(f\"  Train ROC-AUC: {train_metrics['train_roc_auc']:.4f}\")\n",
    "        print(f\"  Val   ROC-AUC: {val_metrics['val_roc_auc']:.4f}\")\n",
    "        print(f\"  Test  ROC-AUC: {test_metrics['test_roc_auc']:.4f}\")\n",
    "        print(f\"  OOT   ROC-AUC: {oot_metrics['oot_roc_auc']:.4f}\")\n",
    "        \n",
    "        # Create and log visualizations\n",
    "        print(f\"\\n[INFO] Creating visualizations...\")\n",
    "        \n",
    "        # ROC curves for each dataset\n",
    "        for dataset_name, y_true, y_proba in [\n",
    "            ('train', y_train, train_proba),\n",
    "            ('val', y_val, val_proba),\n",
    "            ('test', y_test, test_proba),\n",
    "            ('oot', y_oot, oot_proba)\n",
    "        ]:\n",
    "            fig = plot_roc_curve(y_true, y_proba, title=f\"{model_name} - {dataset_name.upper()} ROC Curve\")\n",
    "            mlflow.log_figure(fig, f\"roc_curve_{dataset_name}.png\")\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # Confusion matrices for validation and test sets\n",
    "        for dataset_name, y_true, y_pred in [\n",
    "            ('val', y_val, val_pred),\n",
    "            ('test', y_test, test_pred),\n",
    "            ('oot', y_oot, oot_pred)\n",
    "        ]:\n",
    "            fig = plot_confusion_matrix(y_true, y_pred, \n",
    "                                       title=f\"{model_name} - {dataset_name.upper()} Confusion Matrix\")\n",
    "            mlflow.log_figure(fig, f\"confusion_matrix_{dataset_name}.png\")\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # Log model\n",
    "        print(f\"\\n[INFO] Logging model to MLflow...\")\n",
    "        mlflow.sklearn.log_model(best_model, f\"{model_name}_model\")\n",
    "        \n",
    "        # Log CV results dataframe\n",
    "        cv_results_path = f\"/tmp/{model_name}_cv_results.csv\"\n",
    "        cv_results.to_csv(cv_results_path, index=False)\n",
    "        mlflow.log_artifact(cv_results_path, \"cv_results\")\n",
    "        \n",
    "        print(f\"\\n[SUCCESS] {model_name} training complete!\")\n",
    "        \n",
    "    return best_model, all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Logistic Regression (with One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression with class weights\n",
    "lr_base = LogisticRegression(\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train with hyperparameter tuning\n",
    "lr_model, lr_metrics = train_and_log_model(\n",
    "    model_name=\"LogisticRegression\",\n",
    "    base_model=lr_base,\n",
    "    param_distributions=lr_param_dist,\n",
    "    X_train=X_train_lr,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_lr,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test_lr,\n",
    "    y_test=y_test,\n",
    "    X_oot=X_oot_lr,\n",
    "    y_oot=y_oot,\n",
    "    n_iter=N_ITER,\n",
    "    cv=cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train XGBoost (with Label Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for XGBoost (handles class imbalance)\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"XGBoost scale_pos_weight: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost\n",
    "xgb_base = XGBClassifier(\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train with hyperparameter tuning\n",
    "xgb_model, xgb_metrics = train_and_log_model(\n",
    "    model_name=\"XGBoost\",\n",
    "    base_model=xgb_base,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    X_train=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_tree,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test_tree,\n",
    "    y_test=y_test,\n",
    "    X_oot=X_oot_tree,\n",
    "    y_oot=y_oot,\n",
    "    n_iter=N_ITER,\n",
    "    cv=cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Random Forest (with Label Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest with class weights\n",
    "rf_base = RandomForestClassifier(\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train with hyperparameter tuning\n",
    "rf_model, rf_metrics = train_and_log_model(\n",
    "    model_name=\"RandomForest\",\n",
    "    base_model=rf_base,\n",
    "    param_distributions=rf_param_dist,\n",
    "    X_train=X_train_tree,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_tree,\n",
    "    y_val=y_val,\n",
    "    X_test=X_test_tree,\n",
    "    y_test=y_test,\n",
    "    X_oot=X_oot_tree,\n",
    "    y_oot=y_oot,\n",
    "    n_iter=N_ITER,\n",
    "    cv=cv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'XGBoost', 'Random Forest'],\n",
    "    'Train ROC-AUC': [\n",
    "        lr_metrics['train_roc_auc'],\n",
    "        xgb_metrics['train_roc_auc'],\n",
    "        rf_metrics['train_roc_auc']\n",
    "    ],\n",
    "    'Val ROC-AUC': [\n",
    "        lr_metrics['val_roc_auc'],\n",
    "        xgb_metrics['val_roc_auc'],\n",
    "        rf_metrics['val_roc_auc']\n",
    "    ],\n",
    "    'Test ROC-AUC': [\n",
    "        lr_metrics['test_roc_auc'],\n",
    "        xgb_metrics['test_roc_auc'],\n",
    "        rf_metrics['test_roc_auc']\n",
    "    ],\n",
    "    'OOT ROC-AUC': [\n",
    "        lr_metrics['oot_roc_auc'],\n",
    "        xgb_metrics['oot_roc_auc'],\n",
    "        rf_metrics['oot_roc_auc']\n",
    "    ],\n",
    "    'Test F1': [\n",
    "        lr_metrics['test_f1'],\n",
    "        xgb_metrics['test_f1'],\n",
    "        rf_metrics['test_f1']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best model\n",
    "best_model_idx = comparison_df['Test ROC-AUC'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nüèÜ Best Model (by Test ROC-AUC): {best_model_name}\")\n",
    "print(f\"   Test ROC-AUC: {comparison_df.loc[best_model_idx, 'Test ROC-AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# ROC-AUC comparison across datasets\n",
    "datasets = ['Train', 'Val', 'Test', 'OOT']\n",
    "x = np.arange(len(datasets))\n",
    "width = 0.25\n",
    "\n",
    "axes[0].bar(x - width, comparison_df.iloc[:, 1:5].iloc[0], width, label='Logistic Regression')\n",
    "axes[0].bar(x, comparison_df.iloc[:, 1:5].iloc[1], width, label='XGBoost')\n",
    "axes[0].bar(x + width, comparison_df.iloc[:, 1:5].iloc[2], width, label='Random Forest')\n",
    "axes[0].set_xlabel('Dataset')\n",
    "axes[0].set_ylabel('ROC-AUC')\n",
    "axes[0].set_title('Model Comparison: ROC-AUC Across Datasets')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(datasets)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "\n",
    "# Test set metrics comparison\n",
    "test_metrics_data = {\n",
    "    'ROC-AUC': comparison_df['Test ROC-AUC'].values,\n",
    "    'F1': comparison_df['Test F1'].values\n",
    "}\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, test_metrics_data['ROC-AUC'], width, label='ROC-AUC')\n",
    "axes[1].bar(x + width/2, test_metrics_data['F1'], width, label='F1')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Test Set Metrics Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comparison_df['Model'], rotation=15, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Log comparison to MLflow\n",
    "with mlflow.start_run(run_name=\"Model_Comparison_Summary\"):\n",
    "    mlflow.log_figure(fig, \"model_comparison.png\")\n",
    "    comparison_df.to_csv('/tmp/model_comparison.csv', index=False)\n",
    "    mlflow.log_artifact('/tmp/model_comparison.csv')\n",
    "    mlflow.log_metric(\"best_test_roc_auc\", comparison_df['Test ROC-AUC'].max())\n",
    "    mlflow.set_tag(\"best_model\", best_model_name)\n",
    "\n",
    "print(\"\\n‚úÖ Model comparison logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Review MLflow UI**: Open `http://mlflow:5000` to explore:\n",
    "   - All hyperparameter combinations tried\n",
    "   - Metrics across different datasets\n",
    "   - Model artifacts and visualizations\n",
    "\n",
    "2. **Model Selection**: Based on the comparison above, select your best model for production\n",
    "\n",
    "3. **Model Registry**: Register the best model in MLflow Model Registry:\n",
    "   ```python\n",
    "   # Example:\n",
    "   model_uri = f\"runs:/<run_id>/XGBoost_model\"\n",
    "   mlflow.register_model(model_uri, \"kkbox-churn-predictor\")\n",
    "   ```\n",
    "\n",
    "4. **Further Tuning**: If needed:\n",
    "   - Increase `N_ITER` for more thorough search\n",
    "   - Try different threshold values for precision/recall trade-off\n",
    "   - Perform feature engineering based on model insights\n",
    "   - Consider ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
