{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f02248c-5292-4c29-a62b-0d0f94aeb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc81dff3-ac88-4ddc-a10a-10c84e9bb795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking URI: http://mlflow:5000\n",
      "MLflow Experiment: 2\n"
     ]
    }
   ],
   "source": [
    "# MLflow configuration\n",
    "mlflow_tracking_uri = 'http://mlflow:5000'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(\"kkbox-churn-prediction-fake\")\n",
    "\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"MLflow Experiment: {mlflow.get_experiment_by_name('kkbox-churn-prediction-fake').experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd3279e3-b140-46f4-9856-291765ed785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the \"utils\" folder to PYTHONPATH (works in notebooks)\n",
    "sys.path.append(str(Path().resolve().parent.parent / \"utils\"))\n",
    "from model_preprocessor import preprocess_features_for_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85529c43-cc4b-4826-b8fa-89916c3371ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNAPSHOT = \"2016-04-01\"\n",
    "FEATURE_BASE = \"/app/datamart/gold/feature_store/2016-04-01\"\n",
    "PARTITION_PATH = os.path.join(FEATURE_BASE, f\"snapshot_date={SNAPSHOT}\")\n",
    "MODEL_PKL = \"/app/mlflow/models/lr_churn_model_latest.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56b083d4-33be-4222-ae06-fc7394cd4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = \"/app/datamart/gold/feature_store/snapshot_date=\"+\"2016-04-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30b1c85d-0d9d-4f3c-92a2-f214446d86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Init Spark\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Inference\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3e3bc32-b28c-4e9f-9fc1-4ca2a699414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- msno: string (nullable = true)\n",
      " |-- city_clean: integer (nullable = true)\n",
      " |-- registered_via: integer (nullable = true)\n",
      " |-- registration_date: date (nullable = true)\n",
      " |-- tenure_days_at_snapshot: integer (nullable = true)\n",
      " |-- registered_via_freq: double (nullable = true)\n",
      " |-- city_freq: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- num_unq_w30_sum: long (nullable = true)\n",
      " |-- sum_secs_w30: double (nullable = true)\n",
      " |-- active_days_w30: long (nullable = true)\n",
      " |-- complete_rate_w30: double (nullable = true)\n",
      " |-- sum_secs_w7: double (nullable = true)\n",
      " |-- engagement_ratio_7_30: double (nullable = true)\n",
      " |-- days_since_last_play: integer (nullable = true)\n",
      " |-- trend_secs_w30: double (nullable = true)\n",
      " |-- tenure_days: integer (nullable = true)\n",
      " |-- last_is_auto_renew: integer (nullable = true)\n",
      " |-- last_plan_list_price: integer (nullable = true)\n",
      " |-- auto_renew_share: double (nullable = true)\n",
      "\n",
      "Rows: 2\n",
      "+--------------------------------------------+----------+--------------+-----------------+-----------------------+-------------------+---------+----+-----+---------------+------------+---------------+------------------+------------------+---------------------+--------------------+-------------------+-----------+------------------+--------------------+----------------+\n",
      "|msno                                        |city_clean|registered_via|registration_date|tenure_days_at_snapshot|registered_via_freq|city_freq|year|month|num_unq_w30_sum|sum_secs_w30|active_days_w30|complete_rate_w30 |sum_secs_w7       |engagement_ratio_7_30|days_since_last_play|trend_secs_w30     |tenure_days|last_is_auto_renew|last_plan_list_price|auto_renew_share|\n",
      "+--------------------------------------------+----------+--------------+-----------------+-----------------------+-------------------+---------+----+-----+---------------+------------+---------------+------------------+------------------+---------------------+--------------------+-------------------+-----------+------------------+--------------------+----------------+\n",
      "|jn/lbZ3oyU0QhViguMecdTZbmG49VQp3C8H3DXI70to=|NULL      |NULL          |NULL             |NULL                   |NULL               |NULL     |NULL|NULL |289            |66336.863   |20             |0.8615916955017301|26987.243000000002|0.40682121191048787  |0                   |58.59664285714286  |0          |0                 |0                   |0.0             |\n",
      "|kRqgRrKQ/dPESVdL9W9yUyzYu8JMi7exmtJ6VoZg3hA=|NULL      |NULL          |NULL             |NULL                   |NULL               |NULL     |NULL|NULL |19             |4431.644    |6              |0.8947368421052632|489.195           |0.11038680002274551  |4                   |-222.87428571428575|0          |1                 |149                 |1.0             |\n",
      "+--------------------------------------------+----------+--------------+-----------------+-----------------------+-------------------+---------+----+-----+---------------+------------+---------------+------------------+------------------+---------------------+--------------------+-------------------+-----------+------------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load parquet\n",
    "df = spark.read.parquet(feature_store)\n",
    "\n",
    "# Quick checks\n",
    "df.printSchema()\n",
    "print(\"Rows:\", df.count())\n",
    "df.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01b9fc2f-6a3f-4251-934d-00d0cf410941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model type: XGBoost\n",
      "Num features expected: 38\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../05_model_training/models/xgb_model_20251103_122821.pkl\", \"rb\") as f:\n",
    "    artifact = pickle.load(f)\n",
    "\n",
    "# unpack components\n",
    "model = artifact[\"model\"]\n",
    "scaler = artifact[\"scaler\"]\n",
    "feature_cols = artifact[\"feature_columns\"]\n",
    "numeric_cols = artifact.get(\"numeric_columns\", [])\n",
    "print(\"Loaded model type:\", artifact[\"model_type\"])\n",
    "print(\"Num features expected:\", len(feature_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ef7cf29-1fec-40e8-9430-e4b33549814e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        msno snapshot_date  churn_proba\n",
      "jn/lbZ3oyU0QhViguMecdTZbmG49VQp3C8H3DXI70to=    2016-04-01     0.576900\n",
      "kRqgRrKQ/dPESVdL9W9yUyzYu8JMi7exmtJ6VoZg3hA=    2016-04-01     0.221884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7852/899690542.py:97: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.20980383 -0.52112375]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, scaler_cols] = scaler.transform(X[scaler_cols])\n",
      "/tmp/ipykernel_7852/899690542.py:97: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.50265896  0.30676283]' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, scaler_cols] = scaler.transform(X[scaler_cols])\n",
      "/tmp/ipykernel_7852/899690542.py:97: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.32971791 -0.32971791]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, scaler_cols] = scaler.transform(X[scaler_cols])\n"
     ]
    }
   ],
   "source": [
    "# robust_inference.py — drop-in snippet\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# CONFIG\n",
    "SNAPSHOT = \"2016-04-01\"\n",
    "FEATURE_BASE = \"/app/datamart/gold/feature_store/\"\n",
    "PARTITION = os.path.join(FEATURE_BASE, f\"snapshot_date={SNAPSHOT}\")\n",
    "MODEL_PKL = \"/app/mlflow/models/lr_churn_model_latest.pkl\"\n",
    "\n",
    "# Start Spark (quiet)\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local[*]\").appName(\"inference_safe\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Load partition (fallback to base+filter)\n",
    "if os.path.exists(PARTITION):\n",
    "    sdf = spark.read.parquet(PARTITION)\n",
    "else:\n",
    "    if not os.path.exists(FEATURE_BASE):\n",
    "        raise FileNotFoundError(FEATURE_BASE)\n",
    "    sdf = spark.read.parquet(FEATURE_BASE)\n",
    "    if \"snapshot_date\" in sdf.columns:\n",
    "        sdf = sdf.filter(col(\"snapshot_date\") == SNAPSHOT)\n",
    "\n",
    "if sdf.count() == 0:\n",
    "    raise ValueError(\"No rows for snapshot\")\n",
    "\n",
    "pdf = sdf.toPandas()\n",
    "\n",
    "# Ensure id/snapshot_date exist\n",
    "if \"snapshot_date\" not in pdf.columns:\n",
    "    pdf[\"snapshot_date\"] = SNAPSHOT\n",
    "# rename common id synonyms to msno\n",
    "for c in list(pdf.columns):\n",
    "    if c.lower() in (\"msno\", \"member_id\", \"user_id\", \"userid\", \"id\", \"memberid\") and c != \"msno\":\n",
    "        pdf = pdf.rename(columns={c: \"msno\"})\n",
    "        break\n",
    "if \"msno\" not in pdf.columns:\n",
    "    raise KeyError(\"msno not found\")\n",
    "\n",
    "# Load model artifact (dict)\n",
    "artifact = joblib.load(MODEL_PKL)\n",
    "model = artifact.get(\"model\")\n",
    "scaler = artifact.get(\"scaler\", None)\n",
    "feature_cols = artifact.get(\"feature_columns\")           # final OHE columns expected by model\n",
    "numeric_cols_meta = artifact.get(\"numeric_columns\", [])  # numeric columns list saved at training\n",
    "orig_feature_list = artifact.get(\"original_feature_columns\") or artifact.get(\"original_feature_list\")\n",
    "\n",
    "# --- Build X aligned to feature_cols ---\n",
    "if feature_cols:\n",
    "    # Determine raw categorical columns to OHE:\n",
    "    if orig_feature_list:\n",
    "        # choose candidates that actually exist in pdf\n",
    "        candidates = [c for c in orig_feature_list if c in pdf.columns]\n",
    "    else:\n",
    "        candidates = [c for c in (\"registered_via\", \"city_clean\") if c in pdf.columns]\n",
    "\n",
    "    # pick those candidate columns that are non-numeric (categorical)\n",
    "    cat_cols = [c for c in candidates if not pd.api.types.is_numeric_dtype(pdf[c])]\n",
    "\n",
    "    # Run one-hot (drop_first=True to match training)\n",
    "    if cat_cols:\n",
    "        pdf_ohe = pd.get_dummies(pdf, columns=cat_cols, drop_first=True, dtype=int)\n",
    "    else:\n",
    "        pdf_ohe = pdf.copy()\n",
    "\n",
    "    # Add any missing expected model cols with zeros\n",
    "    for c in feature_cols:\n",
    "        if c not in pdf_ohe.columns:\n",
    "            pdf_ohe[c] = 0\n",
    "\n",
    "    # Keep only model columns (in saved order)\n",
    "    X = pdf_ohe[feature_cols].copy()\n",
    "\n",
    "else:\n",
    "    # No final feature list — fallback to numeric-only selection excluding id/date\n",
    "    drop = {\"msno\", \"snapshot_date\"}\n",
    "    X = pdf[[c for c in pdf.columns if c not in drop and pd.api.types.is_numeric_dtype(pdf[c])]].copy()\n",
    "    if X.shape[1] == 0:\n",
    "        raise RuntimeError(\"No usable features and no feature_columns in pickle\")\n",
    "\n",
    "# --- Convert to numeric safely and fill NaNs ---\n",
    "# coerce every column to numeric (non-numeric -> NaN), then fill with 0\n",
    "for c in X.columns:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# --- Apply scaler safely ---\n",
    "if scaler is not None:\n",
    "    try:\n",
    "        # prefer scaler.feature_names_in_ if available (ensures correct order)\n",
    "        if hasattr(scaler, \"feature_names_in_\"):\n",
    "            scaler_cols = [c for c in scaler.feature_names_in_ if c in X.columns]\n",
    "            if scaler_cols:\n",
    "                X.loc[:, scaler_cols] = scaler.transform(X[scaler_cols])\n",
    "        elif numeric_cols_meta:\n",
    "            scaler_cols = [c for c in numeric_cols_meta if c in X.columns]\n",
    "            if scaler_cols:\n",
    "                X.loc[:, scaler_cols] = scaler.transform(X[scaler_cols])\n",
    "        else:\n",
    "            # fallback: scale all numeric columns\n",
    "            num_cols = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "            if num_cols:\n",
    "                X.loc[:, num_cols] = scaler.transform(X[num_cols])\n",
    "    except Exception as e:\n",
    "        # scaling failed — safe fallback: continue with filled but unscaled features\n",
    "        print(\"Warning: scaler.transform failed:\", str(e))\n",
    "\n",
    "# Final safety net: ensure no NaN remains\n",
    "if X.isnull().values.any():\n",
    "    X = X.fillna(0.0)\n",
    "\n",
    "# --- Predict probabilities ---\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "elif hasattr(model, \"predict\"):\n",
    "    print(\"predict_proba not available; using predict as fallback\")\n",
    "    probs = pd.Series(model.predict(X)).astype(float).values\n",
    "else:\n",
    "    raise RuntimeError(\"Model has no predict/probability methods\")\n",
    "\n",
    "# Attach and show\n",
    "pdf[\"churn_proba\"] = probs\n",
    "print(pdf[[\"msno\", \"snapshot_date\", \"churn_proba\"]].to_string(index=False))\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63ab44f7-95be-4b3e-8330-abae5100543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-------------+------------------+-----------------------------+\n",
      "|msno                                        |snapshot_date|churn_proba       |model_file                   |\n",
      "+--------------------------------------------+-------------+------------------+-----------------------------+\n",
      "|kRqgRrKQ/dPESVdL9W9yUyzYu8JMi7exmtJ6VoZg3hA=|2016-04-01   |0.4923136532306671|xgb_model_20251103_122821.pkl|\n",
      "|jn/lbZ3oyU0QhViguMecdTZbmG49VQp3C8H3DXI70to=|2016-04-01   |0.5832123756408691|xgb_model_20251103_122821.pkl|\n",
      "|kRqgRrKQ/dPESVdL9W9yUyzYu8JMi7exmtJ6VoZg3hA=|2016-04-01   |0.2218837027480165|lr_churn_model_latest.pkl    |\n",
      "|jn/lbZ3oyU0QhViguMecdTZbmG49VQp3C8H3DXI70to=|2016-04-01   |0.5768996750374289|lr_churn_model_latest.pkl    |\n",
      "+--------------------------------------------+-------------+------------------+-----------------------------+\n",
      "\n",
      "root\n",
      " |-- msno: string (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      " |-- churn_proba: double (nullable = true)\n",
      " |-- model_file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "pred_path = \"/app/datamart/gold/predictions/predictions_2016_04_01.parquet\"  # change if needed\n",
    "df_pred = spark.read.parquet(pred_path)\n",
    "\n",
    "df_pred.show(20, truncate=False)\n",
    "df_pred.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8dd1e-55b5-4b61-98be-9f9530de9f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
