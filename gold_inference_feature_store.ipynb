{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483c2ccd-b038-4289-a22e-58abab5bce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ğŸ“¦ Imports and setup\n",
    "# ===============================\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712b7677-8ecc-4f9e-bbf3-1502f3ad36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# âš™ï¸ Initialize Spark\n",
    "# ===============================\n",
    "def initialize_spark():\n",
    "    spark = pyspark.sql.SparkSession.builder \\\n",
    "        .appName(\"gold_feature_store_daily\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4563f2ac-fb2e-4f14-81d7-aab2f0dfd02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ğŸ“¥ Load Silver Tables\n",
    "# ===============================\n",
    "def load_tables(spark):\n",
    "    print(\"Loading Silver layer tables...\\n\")\n",
    "\n",
    "    df_userlogs = spark.read.parquet(\"/app/datamart/silver/user_logs\")\n",
    "    df_transactions = spark.read.parquet(\"/app/datamart/silver/transactions\")\n",
    "    df_latest_transactions = spark.read.parquet(\"/app/datamart/silver/latest_transactions\")\n",
    "    df_members = spark.read.parquet(\"/app/datamart/silver/members\")\n",
    "\n",
    "    return df_userlogs, df_transactions, df_latest_transactions, df_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43fd86e2-e8c5-4afa-9c55-4d18b3dcb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ğŸ‘¤ Registered Users\n",
    "# ===============================\n",
    "def get_registered_users(df_members, inference_date):\n",
    "    registered_users = (\n",
    "        df_members\n",
    "        .withColumn(\"registration_date\", F.to_date(\"registration_date\"))\n",
    "        .filter(F.col(\"registration_date\") <= F.to_date(F.lit(inference_date)))\n",
    "        .withColumn(\n",
    "            \"tenure_days_at_snapshot\",\n",
    "            F.datediff(F.to_date(F.lit(inference_date)), F.col(\"registration_date\"))\n",
    "        )\n",
    "        .select(\"msno\", \"registration_date\", \"tenure_days_at_snapshot\",\n",
    "                \"registered_via\", \"city_clean\")\n",
    "    )\n",
    "\n",
    "    registered_users = (\n",
    "        registered_users\n",
    "        .withColumn(\"registered_via_oh\", F.coalesce(F.col(\"registered_via\"), F.lit(0)))\n",
    "        .withColumn(\"city_clean_oh\", F.coalesce(F.col(\"city_clean\"), F.lit(0)))\n",
    "        .select(\"msno\", \"registration_date\", \"tenure_days_at_snapshot\",\n",
    "                \"registered_via_oh\", \"city_clean_oh\")\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ“„ Registered users up to {inference_date}: {registered_users.count()}\")\n",
    "    return registered_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296f856f-7cb5-42f7-885a-e123624f34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ğŸ§ Filter User Logs by Date Windows\n",
    "# ===============================\n",
    "def filter_userlogs(df_userlogs, inference_date):\n",
    "    ref_today = F.to_date(F.lit(inference_date))\n",
    "    lower30 = F.date_sub(ref_today, 29)\n",
    "    lower7 = F.date_sub(ref_today, 6)\n",
    "\n",
    "    df_userlogs = df_userlogs.withColumn(\"date\", F.to_date(\"date\"))\n",
    "\n",
    "    userlogs_30d = df_userlogs.filter((F.col(\"date\") >= lower30) & (F.col(\"date\") <= ref_today))\n",
    "    userlogs_7d = df_userlogs.filter((F.col(\"date\") >= lower7) & (F.col(\"date\") <= ref_today))\n",
    "\n",
    "    return userlogs_30d, userlogs_7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ad7119-8e4c-458d-bf63-d1dca01d4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ğŸ§® User Log Features\n",
    "# ===============================\n",
    "def add_userlog_features(registered_users, userlogs_30d, userlogs_7d, df_userlogs, inference_date):\n",
    "    # Sum seconds (30d)\n",
    "    user_sum_30d = userlogs_30d.groupBy(\"msno\").agg(F.sum(\"total_secs\").alias(\"sum_secs_w30\"))\n",
    "    registered_users = registered_users.join(user_sum_30d, \"msno\", \"left\").na.fill({\"sum_secs_w30\": 0.0})\n",
    "\n",
    "    # Active days (30d)\n",
    "    user_active_days_30d = userlogs_30d.groupBy(\"msno\").agg(F.countDistinct(\"date\").alias(\"active_days_w30\"))\n",
    "    registered_users = registered_users.join(user_active_days_30d, \"msno\", \"left\").na.fill({\"active_days_w30\": 0})\n",
    "\n",
    "    # Completion rate (30d)\n",
    "    user_complete_rate_30d = userlogs_30d.groupBy(\"msno\").agg(\n",
    "        (F.sum(\"num_100\") / F.when(F.sum(\"num_unq\") > 0, F.sum(\"num_unq\")).otherwise(F.lit(1))).alias(\"complete_rate_w30\")\n",
    "    )\n",
    "    registered_users = registered_users.join(user_complete_rate_30d, \"msno\", \"left\").na.fill({\"complete_rate_w30\": 0.0})\n",
    "\n",
    "    # Sum seconds (7d)\n",
    "    user_sum_7d = userlogs_7d.groupBy(\"msno\").agg(F.sum(\"total_secs\").alias(\"sum_secs_w7\"))\n",
    "    registered_users = registered_users.join(user_sum_7d, \"msno\", \"left\").na.fill({\"sum_secs_w7\": 0.0})\n",
    "\n",
    "    # Engagement ratio\n",
    "    registered_users = registered_users.withColumn(\n",
    "        \"engagement_ratio_7_30\",\n",
    "        F.col(\"sum_secs_w7\") / F.when(F.col(\"sum_secs_w30\") > 0, F.col(\"sum_secs_w30\")).otherwise(F.lit(1))\n",
    "    )\n",
    "\n",
    "    # Days since last play\n",
    "    last_play = (\n",
    "        df_userlogs.filter(F.col(\"date\") <= F.lit(inference_date))\n",
    "        .groupBy(\"msno\")\n",
    "        .agg(F.max(\"date\").alias(\"last_play_date\"))\n",
    "    )\n",
    "    registered_users = registered_users.join(last_play, \"msno\", \"left\") \\\n",
    "        .withColumn(\"days_since_last_play\", F.datediff(F.lit(inference_date), F.col(\"last_play_date\")))\n",
    "\n",
    "    # Trend in total_secs (30d)\n",
    "    daily_secs = userlogs_30d.groupBy(\"msno\", \"date\").agg(F.sum(\"total_secs\").alias(\"daily_secs\"))\n",
    "    window_spec = Window.partitionBy(\"msno\").orderBy(\"date\")\n",
    "    daily_secs = daily_secs.withColumn(\"day_idx\", F.row_number().over(window_spec))\n",
    "    trend = daily_secs.groupBy(\"msno\").agg(\n",
    "        (F.covar_pop(\"day_idx\", \"daily_secs\") / F.var_pop(\"day_idx\")).alias(\"trend_secs_w30\")\n",
    "    )\n",
    "    registered_users = registered_users.join(trend, \"msno\", \"left\").na.fill({\"trend_secs_w30\": 0.0})\n",
    "\n",
    "    return registered_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "153723b9-e296-4d6c-a791-2ed6ab25f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ğŸ’³ Transaction Features\n",
    "# ===============================\n",
    "def add_transaction_features(registered_users, df_transactions, df_latest_transactions, inference_date):\n",
    "    df_transactions_filtered = df_transactions.filter(\n",
    "        F.to_date(F.col(\"transaction_date\")) <= F.to_date(F.lit(inference_date))\n",
    "    )\n",
    "\n",
    "    registered_users = registered_users.join(df_latest_transactions, \"msno\", \"left\") \\\n",
    "        .withColumn(\"tenure_days\", F.datediff(F.col(\"transaction_date\"), F.col(\"registration_date\"))) \\\n",
    "        .na.fill({\"tenure_days\": 0})\n",
    "\n",
    "    auto_renew_stats = (\n",
    "        df_transactions_filtered.groupBy(\"msno\")\n",
    "        .agg(\n",
    "            F.sum(F.when(F.col(\"is_auto_renew\") == 1, 1).otherwise(0)).alias(\"auto_renew_count\"),\n",
    "            F.count(\"*\").alias(\"total_tx_before_expire\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"auto_renew_share\",\n",
    "            F.col(\"auto_renew_count\") / F.when(F.col(\"total_tx_before_expire\") > 0, F.col(\"total_tx_before_expire\")).otherwise(F.lit(1))\n",
    "        )\n",
    "    )\n",
    "    registered_users = registered_users.join(auto_renew_stats.select(\"msno\", \"auto_renew_share\"), \"msno\", \"left\") \\\n",
    "        .na.fill({\"auto_renew_share\": 0.0})\n",
    "\n",
    "    last_is_auto_renew = df_latest_transactions.select(\"msno\", \"is_auto_renew\").withColumnRenamed(\"is_auto_renew\", \"last_is_auto_renew\")\n",
    "    registered_users = registered_users.join(last_is_auto_renew, \"msno\", \"left\")\n",
    "\n",
    "    last_list_price = df_latest_transactions.select(\"msno\", \"plan_list_price\").withColumnRenamed(\"plan_list_price\", \"last_plan_list_price\")\n",
    "    registered_users = registered_users.join(last_list_price, \"msno\", \"left\")\n",
    "\n",
    "    return registered_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582520c-aeb1-4ab3-a5a4-b817e98668ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ğŸ§± Create Gold Features\n",
    "# ===============================\n",
    "def create_gold_features(inference_date, spark):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ—“ï¸ Creating Gold Feature Store snapshot for: {inference_date}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    output_path = f\"datamart/gold/inference_feature_store/{inference_date.replace('-', '_')}\"\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"ğŸŸ¡ Skipping {inference_date} â€” already exists.\\n\")\n",
    "        return\n",
    "\n",
    "    df_userlogs, df_transactions, df_latest_transactions, df_members = load_tables(spark)\n",
    "\n",
    "    registered_users = get_registered_users(df_members, inference_date)\n",
    "    userlogs_30d, userlogs_7d = filter_userlogs(df_userlogs, inference_date)\n",
    "    registered_users = add_userlog_features(registered_users, userlogs_30d, userlogs_7d, df_userlogs, inference_date)\n",
    "    registered_users = add_transaction_features(registered_users, df_transactions, df_latest_transactions, inference_date)\n",
    "    registered_users = registered_users.na.fill(0)\n",
    "\n",
    "    registered_users.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(f\"âœ… Features saved to {output_path}\")\n",
    "    print(f\"ğŸ“Š Total records: {registered_users.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56633401-9bbc-4733-a8cd-c3cdd9d557ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/02 10:29:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/02 10:29:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/02 10:29:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ Generating DAILY Gold Feature Store snapshots\n",
      "ğŸ“† Date range: 2015-02-01 â†’ 2015-02-03\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ğŸ“… Snapshot for 2015-02-01\n",
      "\n",
      "============================================================\n",
      "ğŸ—“ï¸ Creating Gold Feature Store snapshot for: 2015-02-01\n",
      "============================================================\n",
      "\n",
      "Loading Silver layer tables...\n",
      "\n",
      "ğŸ“„ Registered users up to 2015-02-01: 458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features saved to datamart/gold/feature_store/2015_02_01\n",
      "ğŸ“Š Total records: 458\n",
      "\n",
      "\n",
      "ğŸ“… Snapshot for 2015-02-02\n",
      "\n",
      "============================================================\n",
      "ğŸ—“ï¸ Creating Gold Feature Store snapshot for: 2015-02-02\n",
      "============================================================\n",
      "\n",
      "Loading Silver layer tables...\n",
      "\n",
      "ğŸ“„ Registered users up to 2015-02-02: 458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features saved to datamart/gold/feature_store/2015_02_02\n",
      "ğŸ“Š Total records: 458\n",
      "\n",
      "\n",
      "ğŸ“… Snapshot for 2015-02-03\n",
      "\n",
      "============================================================\n",
      "ğŸ—“ï¸ Creating Gold Feature Store snapshot for: 2015-02-03\n",
      "============================================================\n",
      "\n",
      "Loading Silver layer tables...\n",
      "\n",
      "ğŸ“„ Registered users up to 2015-02-03: 458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features saved to datamart/gold/feature_store/2015_02_03\n",
      "ğŸ“Š Total records: 458\n",
      "\n",
      "\n",
      "âœ… Completed all daily feature stores.\n",
      "Total snapshots created: 3\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ğŸš€ Main: Generate Daily Snapshots\n",
    "# ===============================\n",
    "def main():\n",
    "    spark = initialize_spark()\n",
    "\n",
    "    start_date = datetime(2015, 2, 1)\n",
    "    end_date = datetime(2015, 2, 3)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸš€ Generating DAILY Gold Feature Store snapshots\")\n",
    "    print(f\"ğŸ“† Date range: {start_date.strftime('%Y-%m-%d')} â†’ {end_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    current = start_date\n",
    "    total_days = 0\n",
    "\n",
    "    while current <= end_date:\n",
    "        inference_date = current.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"\\nğŸ“… Snapshot for {inference_date}\")\n",
    "        try:\n",
    "            create_gold_features(inference_date, spark)\n",
    "            total_days += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed for {inference_date}: {e}\")\n",
    "\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "    print(f\"\\nâœ… Completed all daily feature stores.\")\n",
    "    print(f\"Total snapshots created: {total_days}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "# Run main\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f55158-8c22-42ed-903d-ce16cdb16e08",
   "metadata": {},
   "source": [
    "import os\n",
    "os.system(\"pkill -f 'org.apache.spark.deploy'\")  # kill any Spark JVMs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
